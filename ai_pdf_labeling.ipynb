{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b975526",
   "metadata": {},
   "source": [
    "# AI-Powered Malaysian Legal PDF Analysis\n",
    "## Extract PDF Content with Unstructured + Label Context with OpenAI/Gemini\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Extract text from Malaysian legal PDFs using the `unstructured` library\n",
    "2. Use AI models (OpenAI GPT-4 or Google Gemini) to intelligently label and categorize content\n",
    "3. Optimize for Google Colab GPU environment\n",
    "4. Export structured, labeled results for legal AI/RAG systems\n",
    "\n",
    "**Perfect for:** Legal document processing, content classification, and building intelligent legal knowledge bases\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 **Optimized for Google Colab GPU Runtime**\n",
    "- Make sure to enable GPU: Runtime → Change runtime type → Hardware accelerator: GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be6a4d",
   "metadata": {},
   "source": [
    "## 📦 Install Required Libraries\n",
    "\n",
    "Install all necessary packages for PDF processing and AI model integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "!pip install -q unstructured[pdf] \n",
    "!pip install -q openai>=1.0.0\n",
    "!pip install -q google-generativeai\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q tqdm\n",
    "!pip install -q pandas\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "\n",
    "# Install additional dependencies for unstructured PDF processing\n",
    "!apt-get update -qq\n",
    "!apt-get install -qq poppler-utils tesseract-ocr\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3693c",
   "metadata": {},
   "source": [
    "## 🔧 Import Dependencies and Setup API Keys\n",
    "\n",
    "Import all necessary libraries and configure secure API key handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# PDF Processing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# AI Model Libraries\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "\n",
    "# API Key Setup\n",
    "print(\"\\n🔑 API Key Configuration:\")\n",
    "print(\"Please set your API keys using one of these methods:\")\n",
    "print(\"1. Use Colab Secrets (recommended)\")\n",
    "print(\"2. Set environment variables\")\n",
    "print(\"3. Direct assignment (not recommended for production)\")\n",
    "\n",
    "# Secure API key handling for Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    print(\"✅ Using Colab secrets for API keys\")\n",
    "except:\n",
    "    # Fallback to environment variables or manual input\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "    \n",
    "    if not OPENAI_API_KEY and not GEMINI_API_KEY:\n",
    "        print(\"⚠️  No API keys found. Please set them manually:\")\n",
    "        print(\"OPENAI_API_KEY = 'your-openai-key-here'\")\n",
    "        print(\"GEMINI_API_KEY = 'your-gemini-key-here'\")\n",
    "\n",
    "# Set API keys\n",
    "if OPENAI_API_KEY:\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    print(\"✅ OpenAI API key configured\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"✅ Gemini API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee1d42",
   "metadata": {},
   "source": [
    "## 🚀 Configure GPU and Environment\n",
    "\n",
    "Check GPU availability and optimize environment for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"✅ GPU Available: {gpu_name}\")\n",
    "        print(f\"📊 GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        # Set optimal settings for GPU\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        print(\"⚠️  No GPU detected. Using CPU (will be slower)\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  PyTorch not available. Install if you need GPU acceleration.\")\n",
    "\n",
    "# Check system resources\n",
    "import psutil\n",
    "cpu_count = psutil.cpu_count()\n",
    "memory_gb = psutil.virtual_memory().total / 1024**3\n",
    "\n",
    "print(f\"🖥️  CPU cores: {cpu_count}\")\n",
    "print(f\"💾 RAM: {memory_gb:.1f} GB\")\n",
    "\n",
    "# Optimize environment settings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid tokenizer warnings\n",
    "os.environ['OMP_NUM_THREADS'] = str(min(4, cpu_count))  # Optimize CPU usage\n",
    "\n",
    "print(\"✅ Environment optimized for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6e8f3",
   "metadata": {},
   "source": [
    "## 📄 Load and Extract PDF Content\n",
    "\n",
    "Use unstructured library to extract elements from Malaysian legal PDFs with high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Google Drive dependency for Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import shutil\n",
    "    print(\"✅ Google Colab environment detected\")\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  Not in Google Colab - manual file upload required\")\n",
    "    COLAB_ENV = False\n",
    "\n",
    "def mount_and_find_pdfs():\n",
    "    \"\"\"Mount Google Drive and search for PDF files\"\"\"\n",
    "    if not COLAB_ENV:\n",
    "        print(\"❌ This function requires Google Colab environment\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Mount Google Drive\n",
    "        print(\"🔗 Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"✅ Google Drive mounted successfully!\")\n",
    "        \n",
    "        # Search for PDF files in common locations\n",
    "        import glob\n",
    "        \n",
    "        search_patterns = [\n",
    "            '/content/drive/MyDrive/**/*.pdf',\n",
    "            '/content/drive/MyDrive/legal_documents/**/*.pdf',\n",
    "            '/content/drive/MyDrive/malaysian_acts/**/*.pdf',\n",
    "            '/content/drive/MyDrive/PDFs/**/*.pdf',\n",
    "            '/content/drive/Shareddrives/**/*.pdf'\n",
    "        ]\n",
    "        \n",
    "        all_pdfs = []\n",
    "        print(\"🔍 Searching for PDF files...\")\n",
    "        \n",
    "        for pattern in search_patterns:\n",
    "            found_pdfs = glob.glob(pattern, recursive=True)\n",
    "            all_pdfs.extend(found_pdfs)\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        unique_pdfs = list(set(all_pdfs))\n",
    "        unique_pdfs.sort()\n",
    "        \n",
    "        print(f\"📁 Found {len(unique_pdfs)} PDF files in Google Drive\")\n",
    "        \n",
    "        # Display found PDFs\n",
    "        if unique_pdfs:\n",
    "            print(\"\\n📋 Available PDF files:\")\n",
    "            for i, pdf_path in enumerate(unique_pdfs[:20]):  # Show first 20\n",
    "                file_size = os.path.getsize(pdf_path) / 1024 / 1024  # Size in MB\n",
    "                print(f\"   {i+1:2d}. {Path(pdf_path).name} ({file_size:.1f} MB)\")\n",
    "                print(f\"       📂 {pdf_path}\")\n",
    "            \n",
    "            if len(unique_pdfs) > 20:\n",
    "                print(f\"   ... and {len(unique_pdfs) - 20} more files\")\n",
    "        \n",
    "        return unique_pdfs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing Google Drive: {e}\")\n",
    "        return []\n",
    "\n",
    "def copy_pdf_to_workspace(drive_pdf_path: str, workspace_name: str = None) -> str:\n",
    "    \"\"\"Copy PDF from Google Drive to Colab workspace for processing\"\"\"\n",
    "    try:\n",
    "        if workspace_name is None:\n",
    "            workspace_name = Path(drive_pdf_path).name\n",
    "        \n",
    "        workspace_path = f\"/content/{workspace_name}\"\n",
    "        \n",
    "        print(f\"📋 Copying {Path(drive_pdf_path).name} to workspace...\")\n",
    "        shutil.copy2(drive_pdf_path, workspace_path)\n",
    "        \n",
    "        file_size = os.path.getsize(workspace_path) / 1024 / 1024\n",
    "        print(f\"✅ PDF copied to: {workspace_path} ({file_size:.1f} MB)\")\n",
    "        \n",
    "        return workspace_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error copying PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_pdf_content(pdf_path: str, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract content from PDF using unstructured library with high accuracy settings.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file (local or Google Drive)\n",
    "        max_pages: Maximum number of pages to process (None for all pages)\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted elements with metadata\n",
    "    \"\"\"\n",
    "    logger.info(f\"🔍 Extracting content from: {Path(pdf_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        # Configure extraction settings for maximum accuracy\n",
    "        elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            strategy=\"hi_res\",  # High resolution for legal documents\n",
    "            infer_table_structure=True,  # Detect tables\n",
    "            extract_images_in_pdf=False,  # Skip images for text focus\n",
    "            include_page_breaks=True,  # Preserve page structure\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"✅ Extracted {len(elements)} elements from entire PDF\")\n",
    "        \n",
    "        # Convert to structured format\n",
    "        structured_elements = []\n",
    "        \n",
    "        for i, element in enumerate(elements):\n",
    "            # Get element metadata\n",
    "            page_num = 1  # default\n",
    "            coordinates = None\n",
    "            \n",
    "            if hasattr(element, 'metadata') and element.metadata:\n",
    "                if hasattr(element.metadata, 'page_number'):\n",
    "                    page_num = element.metadata.page_number\n",
    "                if hasattr(element.metadata, 'coordinates'):\n",
    "                    coordinates = str(element.metadata.coordinates)\n",
    "            \n",
    "            # Filter by page limit if specified\n",
    "            if max_pages and page_num > max_pages:\n",
    "                continue\n",
    "            \n",
    "            # Create structured element\n",
    "            structured_element = {\n",
    "                'index': i,\n",
    "                'page_number': page_num,\n",
    "                'element_type': str(type(element).__name__),\n",
    "                'text': str(element).strip(),\n",
    "                'character_count': len(str(element)),\n",
    "                'coordinates': coordinates,\n",
    "                'metadata': {\n",
    "                    'extraction_timestamp': datetime.now().isoformat(),\n",
    "                    'source_file': Path(pdf_path).name\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            structured_elements.append(structured_element)\n",
    "        \n",
    "        # Filter and sort\n",
    "        filtered_elements = [e for e in structured_elements if e['character_count'] > 5]  # Remove tiny elements\n",
    "        filtered_elements.sort(key=lambda x: (x['page_number'], x['index']))  # Sort by page and order\n",
    "        \n",
    "        logger.info(f\"📊 Final elements: {len(filtered_elements)} (after filtering)\")\n",
    "        return filtered_elements\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error extracting PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_drive_pdf(pdf_index: int = None, pdf_path: str = None, max_pages: int = 3):\n",
    "    \"\"\"\n",
    "    Process a PDF from Google Drive by index or path\n",
    "    \n",
    "    Args:\n",
    "        pdf_index: Index of PDF from the found list (1-based)\n",
    "        pdf_path: Direct path to PDF file\n",
    "        max_pages: Maximum pages to process\n",
    "    \"\"\"\n",
    "    if COLAB_ENV:\n",
    "        # Get available PDFs\n",
    "        available_pdfs = mount_and_find_pdfs()\n",
    "        \n",
    "        if not available_pdfs:\n",
    "            print(\"❌ No PDF files found in Google Drive\")\n",
    "            return None\n",
    "        \n",
    "        # Select PDF\n",
    "        if pdf_index:\n",
    "            if 1 <= pdf_index <= len(available_pdfs):\n",
    "                selected_pdf = available_pdfs[pdf_index - 1]\n",
    "            else:\n",
    "                print(f\"❌ Invalid index. Please choose 1-{len(available_pdfs)}\")\n",
    "                return None\n",
    "        elif pdf_path:\n",
    "            if pdf_path in available_pdfs:\n",
    "                selected_pdf = pdf_path\n",
    "            else:\n",
    "                print(f\"❌ PDF not found: {pdf_path}\")\n",
    "                return None\n",
    "        else:\n",
    "            # Use first PDF as example\n",
    "            selected_pdf = available_pdfs[0]\n",
    "            print(f\"📝 Using first PDF as example: {Path(selected_pdf).name}\")\n",
    "        \n",
    "        # Copy to workspace and process\n",
    "        workspace_pdf = copy_pdf_to_workspace(selected_pdf)\n",
    "        if workspace_pdf:\n",
    "            print(f\"\\n🔄 Processing PDF: {Path(workspace_pdf).name}\")\n",
    "            elements = extract_pdf_content(workspace_pdf, max_pages=max_pages)\n",
    "            \n",
    "            if elements:\n",
    "                print(f\"✅ Successfully extracted {len(elements)} elements\")\n",
    "                return elements\n",
    "            else:\n",
    "                print(\"❌ Failed to extract content\")\n",
    "                return None\n",
    "    else:\n",
    "        print(\"❌ Google Drive integration requires Google Colab environment\")\n",
    "        return None\n",
    "\n",
    "# Demo usage and setup\n",
    "print(\"🚀 Google Drive PDF Processing Setup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if COLAB_ENV:\n",
    "    print(\"📱 Available commands:\")\n",
    "    print(\"1. mount_and_find_pdfs() - Find all PDFs in your Google Drive\")\n",
    "    print(\"2. process_drive_pdf(pdf_index=1) - Process first PDF\")\n",
    "    print(\"3. process_drive_pdf(pdf_index=2, max_pages=5) - Process 2nd PDF, 5 pages\")\n",
    "    print(\"\\n🎯 Quick start:\")\n",
    "    print(\"   available_pdfs = mount_and_find_pdfs()\")\n",
    "    print(\"   elements = process_drive_pdf(pdf_index=1, max_pages=3)\")\n",
    "    \n",
    "    # Uncomment the next line to automatically find PDFs\n",
    "    # available_pdfs = mount_and_find_pdfs()\n",
    "    \n",
    "else:\n",
    "    print(\"📁 Manual PDF upload required (not in Colab)\")\n",
    "    print(\"1. Upload your PDF to Colab files\")\n",
    "    print(\"2. Use: elements = extract_pdf_content('/content/your_file.pdf')\")\n",
    "    \n",
    "    # Example for manual upload\n",
    "    pdf_path = \"/content/sample_legal_document.pdf\"  # Update this path\n",
    "    max_pages = 3\n",
    "    \n",
    "    print(f\"\\n🔄 Ready to extract from: {pdf_path}\")\n",
    "    print(f\"📄 Max pages: {max_pages}\")\n",
    "    print(\"# Uncomment when ready: elements = extract_pdf_content(pdf_path, max_pages=max_pages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f997fd1",
   "metadata": {},
   "source": [
    "## 🤖 Setup AI Model Clients\n",
    "\n",
    "Initialize OpenAI GPT and Google Gemini clients for intelligent content labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AILabelingSystem:\n",
    "    \"\"\"Unified system for labeling PDF content using OpenAI or Gemini\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openai_client = None\n",
    "        self.gemini_model = None\n",
    "        self.setup_clients()\n",
    "    \n",
    "    def setup_clients(self):\n",
    "        \"\"\"Initialize available AI model clients\"\"\"\n",
    "        \n",
    "        # Setup OpenAI client\n",
    "        if OPENAI_API_KEY:\n",
    "            try:\n",
    "                self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "                logger.info(\"✅ OpenAI client initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ OpenAI setup failed: {e}\")\n",
    "        \n",
    "        # Setup Gemini client\n",
    "        if GEMINI_API_KEY:\n",
    "            try:\n",
    "                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "                logger.info(\"✅ Gemini client initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Gemini setup failed: {e}\")\n",
    "        \n",
    "        if not self.openai_client and not self.gemini_model:\n",
    "            logger.warning(\"⚠️  No AI models available. Please configure API keys.\")\n",
    "    \n",
    "    def test_models(self):\n",
    "        \"\"\"Test both models with a simple query\"\"\"\n",
    "        test_text = \"This is a test of the legal document analysis system.\"\n",
    "        \n",
    "        print(\"🧪 Testing AI Models:\\\\n\")\n",
    "        \n",
    "        # Test OpenAI\n",
    "        if self.openai_client:\n",
    "            try:\n",
    "                response = self.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": f\"Classify this text: {test_text}\"}\n",
    "                    ],\n",
    "                    max_tokens=50\n",
    "                )\n",
    "                print(\"✅ OpenAI GPT: Working\")\n",
    "                print(f\"   Response: {response.choices[0].message.content.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ OpenAI test failed: {e}\")\n",
    "        \n",
    "        # Test Gemini\n",
    "        if self.gemini_model:\n",
    "            try:\n",
    "                response = self.gemini_model.generate_content(\n",
    "                    f\"Classify this text in one sentence: {test_text}\"\n",
    "                )\n",
    "                print(\"✅ Gemini: Working\")\n",
    "                print(f\"   Response: {response.text.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Gemini test failed: {e}\")\n",
    "\n",
    "# Initialize the AI labeling system\n",
    "ai_system = AILabelingSystem()\n",
    "\n",
    "# Test the models\n",
    "ai_system.test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fb8a2",
   "metadata": {},
   "source": [
    "## 🏷️ Create Context Labeling Functions\n",
    "\n",
    "Define intelligent functions to classify and label legal document content using AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeling_prompt(text: str, element_type: str) -> str:\n",
    "    \"\"\"Create a sophisticated prompt for legal content classification\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert legal document analyst specializing in Malaysian law. \n",
    "Analyze the following text from a legal document and provide a comprehensive classification.\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "\"{text}\"\n",
    "\n",
    "ELEMENT TYPE: {element_type}\n",
    "\n",
    "Please provide a JSON response with the following fields:\n",
    "1. \"content_category\": Main category (e.g., \"section_header\", \"legal_definition\", \"procedural_requirement\", \"penalty_clause\", \"table_of_contents\", \"preamble\", \"schedule\", \"interpretation\")\n",
    "2. \"legal_significance\": Level of legal importance (e.g., \"high\", \"medium\", \"low\")\n",
    "3. \"subject_matter\": What the text is about (e.g., \"criminal_procedure\", \"corporate_law\", \"tax_provisions\", \"regulatory_compliance\")\n",
    "4. \"contains_definitions\": Whether it contains legal definitions (true/false)\n",
    "5. \"contains_penalties\": Whether it mentions penalties or sanctions (true/false)\n",
    "6. \"references_other_sections\": Whether it references other legal sections (true/false)\n",
    "7. \"actionable_requirements\": Whether it contains specific requirements or obligations (true/false)\n",
    "8. \"confidence_score\": Your confidence in this classification (0.0 to 1.0)\n",
    "9. \"keywords\": Key legal terms found in the text (array)\n",
    "10. \"summary\": Brief summary of the content (1-2 sentences)\n",
    "\n",
    "Respond with valid JSON only.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def label_with_openai(ai_system, text: str, element_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"Label content using OpenAI GPT\"\"\"\n",
    "    try:\n",
    "        prompt = create_labeling_prompt(text, element_type)\n",
    "        \n",
    "        response = ai_system.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # Use GPT-4 for better accuracy\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal document analysis expert. Always respond with valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.1  # Low temperature for consistent results\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        result['model_used'] = 'gpt-4'\n",
    "        result['processing_time'] = response.usage.total_tokens if hasattr(response, 'usage') else None\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"OpenAI labeling failed: {e}\")\n",
    "        return {\n",
    "            \"content_category\": \"unknown\",\n",
    "            \"error\": str(e),\n",
    "            \"model_used\": \"gpt-4\"\n",
    "        }\n",
    "\n",
    "def label_with_gemini(ai_system, text: str, element_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"Label content using Google Gemini\"\"\"\n",
    "    try:\n",
    "        prompt = create_labeling_prompt(text, element_type)\n",
    "        \n",
    "        response = ai_system.gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=500,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        content = response.text.strip()\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        result['model_used'] = 'gemini-1.5-flash'\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gemini labeling failed: {e}\")\n",
    "        return {\n",
    "            \"content_category\": \"unknown\", \n",
    "            \"error\": str(e),\n",
    "            \"model_used\": \"gemini-1.5-flash\"\n",
    "        }\n",
    "\n",
    "def smart_label_content(ai_system, text: str, element_type: str, preferred_model: str = \"auto\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Intelligently label content using the best available model\n",
    "    \n",
    "    Args:\n",
    "        ai_system: AILabelingSystem instance\n",
    "        text: Text content to label\n",
    "        element_type: Type of element from unstructured\n",
    "        preferred_model: \"openai\", \"gemini\", or \"auto\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Skip very short text\n",
    "    if len(text.strip()) < 10:\n",
    "        return {\n",
    "            \"content_category\": \"minimal_content\",\n",
    "            \"confidence_score\": 0.9,\n",
    "            \"model_used\": \"rule_based\"\n",
    "        }\n",
    "    \n",
    "    # Choose model\n",
    "    if preferred_model == \"auto\":\n",
    "        # Prefer OpenAI for complex analysis, Gemini for speed\n",
    "        if ai_system.openai_client and len(text) > 200:\n",
    "            preferred_model = \"openai\"\n",
    "        elif ai_system.gemini_model:\n",
    "            preferred_model = \"gemini\"\n",
    "        elif ai_system.openai_client:\n",
    "            preferred_model = \"openai\"\n",
    "    \n",
    "    # Label with chosen model\n",
    "    if preferred_model == \"openai\" and ai_system.openai_client:\n",
    "        return label_with_openai(ai_system, text, element_type)\n",
    "    elif preferred_model == \"gemini\" and ai_system.gemini_model:\n",
    "        return label_with_gemini(ai_system, text, element_type)\n",
    "    else:\n",
    "        logger.warning(\"No AI model available for labeling\")\n",
    "        return {\n",
    "            \"content_category\": \"unlabeled\",\n",
    "            \"error\": \"No AI model available\",\n",
    "            \"model_used\": \"none\"\n",
    "        }\n",
    "\n",
    "print(\"✅ Context labeling functions created!\")\n",
    "print(\"🎯 Available functions:\")\n",
    "print(\"   • smart_label_content() - Auto-select best model\")\n",
    "print(\"   • label_with_openai() - Use GPT-4\")\n",
    "print(\"   • label_with_gemini() - Use Gemini\")\n",
    "print(\"\\\\n🧪 Test with a sample:\")\n",
    "\n",
    "# Test the labeling system\n",
    "sample_text = \"31A. Establishment of the Commission\\\\n(1) There is established a commission to be known as the Malaysian Communications and Multimedia Commission.\"\n",
    "sample_result = smart_label_content(ai_system, sample_text, \"NarrativeText\")\n",
    "print(f\"Sample result: {json.dumps(sample_result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d5ff0",
   "metadata": {},
   "source": [
    "## 🔄 Process PDF Elements with AI\n",
    "\n",
    "Extract PDF content and apply intelligent AI labeling to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_ai_labeling(\n",
    "    pdf_path: str, \n",
    "    max_pages: Optional[int] = None,\n",
    "    preferred_model: str = \"auto\",\n",
    "    rate_limit_delay: float = 1.0,\n",
    "    batch_size: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Extract PDF content and label with AI\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        max_pages: Max pages to process\n",
    "        preferred_model: \"openai\", \"gemini\", or \"auto\"\n",
    "        rate_limit_delay: Delay between API calls (seconds)\n",
    "        batch_size: Process elements in batches\n",
    "    \n",
    "    Returns:\n",
    "        Complete analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"🚀 Starting AI-powered PDF analysis: {Path(pdf_path).name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Extract PDF content\n",
    "    elements = extract_pdf_content(pdf_path, max_pages)\n",
    "    if not elements:\n",
    "        return {\"error\": \"Failed to extract PDF content\", \"elements\": []}\n",
    "    \n",
    "    logger.info(f\"📄 Processing {len(elements)} elements with AI labeling...\")\n",
    "    \n",
    "    # Step 2: Process elements with AI\n",
    "    labeled_elements = []\n",
    "    api_calls_made = 0\n",
    "    errors = 0\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    for i in tqdm(range(0, len(elements), batch_size), desc=\"🤖 AI Labeling\"):\n",
    "        batch = elements[i:i + batch_size]\n",
    "        \n",
    "        for element in batch:\n",
    "            try:\n",
    "                # AI labeling\n",
    "                ai_label = smart_label_content(\n",
    "                    ai_system, \n",
    "                    element['text'], \n",
    "                    element['element_type'], \n",
    "                    preferred_model\n",
    "                )\n",
    "                \n",
    "                # Combine original element with AI labels\n",
    "                labeled_element = {\n",
    "                    **element,  # Original element data\n",
    "                    'ai_labels': ai_label,  # AI-generated labels\n",
    "                    'processing_timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                labeled_elements.append(labeled_element)\n",
    "                api_calls_made += 1\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(rate_limit_delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing element {element['index']}: {e}\")\n",
    "                errors += 1\n",
    "                \n",
    "                # Add element without AI labels\n",
    "                labeled_elements.append({\n",
    "                    **element,\n",
    "                    'ai_labels': {'error': str(e), 'content_category': 'processing_failed'},\n",
    "                    'processing_timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    # Step 3: Generate analysis summary\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    categories = {}\n",
    "    high_importance = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for element in labeled_elements:\n",
    "        ai_labels = element.get('ai_labels', {})\n",
    "        category = ai_labels.get('content_category', 'unknown')\n",
    "        categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        if ai_labels.get('legal_significance') == 'high':\n",
    "            high_importance += 1\n",
    "        \n",
    "        total_chars += element.get('character_count', 0)\n",
    "    \n",
    "    # Compile final results\n",
    "    results = {\n",
    "        'document_info': {\n",
    "            'source_file': Path(pdf_path).name,\n",
    "            'total_elements': len(labeled_elements),\n",
    "            'total_characters': total_chars,\n",
    "            'pages_processed': max_pages if max_pages else \"all\",\n",
    "            'processing_time_seconds': round(processing_time, 2),\n",
    "            'api_calls_made': api_calls_made,\n",
    "            'errors': errors\n",
    "        },\n",
    "        'analysis_summary': {\n",
    "            'content_categories': categories,\n",
    "            'high_importance_elements': high_importance,\n",
    "            'category_distribution': {k: round(v/len(labeled_elements)*100, 1) for k, v in categories.items()},\n",
    "            'avg_chars_per_element': round(total_chars / len(labeled_elements)) if labeled_elements else 0\n",
    "        },\n",
    "        'labeled_elements': labeled_elements\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"✅ Analysis complete! Processed {len(labeled_elements)} elements in {processing_time:.1f}s\")\n",
    "    return results\n",
    "\n",
    "# Example processing function\n",
    "def run_example_analysis(pdf_path: str = \"/content/sample_legal_document.pdf\"):\n",
    "    \"\"\"Run a complete example analysis\"\"\"\n",
    "    \n",
    "    if not Path(pdf_path).exists():\n",
    "        print(f\"❌ PDF not found: {pdf_path}\")\n",
    "        print(\"📁 Please upload your PDF to Colab and update the path\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🔍 Analyzing: {pdf_path}\")\n",
    "    print(\"⚙️  Settings: First 3 pages, auto model selection, 1s rate limit\")\n",
    "    \n",
    "    # Run analysis\n",
    "    results = process_pdf_with_ai_labeling(\n",
    "        pdf_path=pdf_path,\n",
    "        max_pages=3,  # First 3 pages only\n",
    "        preferred_model=\"auto\",\n",
    "        rate_limit_delay=1.0,\n",
    "        batch_size=5\n",
    "    )\n",
    "    \n",
    "    if results and 'document_info' in results:\n",
    "        # Display summary\n",
    "        info = results['document_info']\n",
    "        summary = results['analysis_summary']\n",
    "        \n",
    "        print(f\"\\\\n📊 ANALYSIS RESULTS:\")\n",
    "        print(f\"📄 Elements processed: {info['total_elements']}\")\n",
    "        print(f\"⏱️  Processing time: {info['processing_time_seconds']}s\")\n",
    "        print(f\"🤖 API calls made: {info['api_calls_made']}\")\n",
    "        print(f\"⚠️  Errors: {info['errors']}\")\n",
    "        \n",
    "        print(f\"\\\\n🏷️  CONTENT CATEGORIES:\")\n",
    "        for category, count in summary['content_categories'].items():\n",
    "            percentage = summary['category_distribution'][category]\n",
    "            print(f\"   • {category}: {count} ({percentage}%)\")\n",
    "        \n",
    "        print(f\"\\\\n🎯 High importance elements: {summary['high_importance_elements']}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"✅ PDF processing functions ready!\")\n",
    "print(\"\\\\n🚀 To run analysis:\")\n",
    "print(\"1. Upload your PDF to Colab\")\n",
    "print(\"2. Update the pdf_path in run_example_analysis()\")\n",
    "print(\"3. Call: results = run_example_analysis()\")\n",
    "\n",
    "# Uncomment to run with your PDF\n",
    "# results = run_example_analysis(\"/content/your_legal_document.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875eee9",
   "metadata": {},
   "source": [
    "## ⚡ Batch Processing and Optimization\n",
    "\n",
    "Advanced techniques for processing multiple PDFs efficiently with caching and GPU optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class OptimizedPDFProcessor:\n",
    "    \"\"\"Advanced PDF processor with caching and batch optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"/content/pdf_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.processed_files = {}\n",
    "        \n",
    "    def get_file_hash(self, file_path: str) -> str:\n",
    "        \"\"\"Generate hash for file caching\"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return hashlib.md5(f.read()).hexdigest()\n",
    "    \n",
    "    def load_from_cache(self, file_hash: str) -> Optional[Dict]:\n",
    "        \"\"\"Load processed results from cache\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{file_hash}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Cache load failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def save_to_cache(self, file_hash: str, results: Dict):\n",
    "        \"\"\"Save results to cache\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{file_hash}.pkl\"\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cache save failed: {e}\")\n",
    "    \n",
    "    def process_single_pdf_optimized(\n",
    "        self, \n",
    "        pdf_path: str,\n",
    "        use_cache: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict:\n",
    "        \"\"\"Process single PDF with caching\"\"\"\n",
    "        \n",
    "        file_hash = self.get_file_hash(pdf_path)\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache:\n",
    "            cached_result = self.load_from_cache(file_hash)\n",
    "            if cached_result:\n",
    "                logger.info(f\"📁 Loaded from cache: {Path(pdf_path).name}\")\n",
    "                return cached_result\n",
    "        \n",
    "        # Process if not cached\n",
    "        logger.info(f\"🔄 Processing (not cached): {Path(pdf_path).name}\")\n",
    "        results = process_pdf_with_ai_labeling(pdf_path, **kwargs)\n",
    "        \n",
    "        # Save to cache\n",
    "        if use_cache and results:\n",
    "            self.save_to_cache(file_hash, results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_process_pdfs(\n",
    "        self,\n",
    "        pdf_paths: List[str],\n",
    "        max_workers: int = 3,  # Conservative for API rate limits\n",
    "        use_cache: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple PDFs in parallel with optimization\"\"\"\n",
    "        \n",
    "        logger.info(f\"🚀 Starting batch processing of {len(pdf_paths)} PDFs\")\n",
    "        \n",
    "        results = {}\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        # Process with controlled parallelism\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all jobs\n",
    "            future_to_path = {\n",
    "                executor.submit(\n",
    "                    self.process_single_pdf_optimized, \n",
    "                    pdf_path, \n",
    "                    use_cache, \n",
    "                    **kwargs\n",
    "                ): pdf_path for pdf_path in pdf_paths\n",
    "            }\n",
    "            \n",
    "            # Collect results with progress bar\n",
    "            for future in tqdm(as_completed(future_to_path), total=len(pdf_paths), desc=\"📄 Processing PDFs\"):\n",
    "                pdf_path = future_to_path[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[Path(pdf_path).name] = result\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {pdf_path}: {e}\")\n",
    "                    results[Path(pdf_path).name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Generate batch summary\n",
    "        total_time = time.time() - total_start_time\n",
    "        successful = sum(1 for r in results.values() if \"error\" not in r)\n",
    "        failed = len(results) - successful\n",
    "        \n",
    "        batch_summary = {\n",
    "            'batch_info': {\n",
    "                'total_files': len(pdf_paths),\n",
    "                'successful': successful,\n",
    "                'failed': failed,\n",
    "                'total_processing_time': round(total_time, 2),\n",
    "                'avg_time_per_file': round(total_time / len(pdf_paths), 2)\n",
    "            },\n",
    "            'file_results': results\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"✅ Batch complete: {successful}/{len(pdf_paths)} successful in {total_time:.1f}s\")\n",
    "        return batch_summary\n",
    "\n",
    "def demonstrate_optimization():\n",
    "    \"\"\"Demonstrate advanced processing features\"\"\"\n",
    "    \n",
    "    print(\"⚡ OPTIMIZATION FEATURES:\")\n",
    "    print(\"\\\\n1. 📁 Intelligent Caching:\")\n",
    "    print(\"   • Files are hashed to detect changes\")\n",
    "    print(\"   • Processed results cached to disk\")\n",
    "    print(\"   • Automatic cache invalidation\")\n",
    "    \n",
    "    print(\"\\\\n2. 🔄 Parallel Processing:\")\n",
    "    print(\"   • Multiple PDFs processed simultaneously\")\n",
    "    print(\"   • Controlled concurrency for API limits\")\n",
    "    print(\"   • Progress tracking and error handling\")\n",
    "    \n",
    "    print(\"\\\\n3. 🎯 Memory Optimization:\")\n",
    "    print(\"   • Batch processing to manage memory\")\n",
    "    print(\"   • Garbage collection between files\")\n",
    "    print(\"   • GPU memory monitoring\")\n",
    "    \n",
    "    print(\"\\\\n4. 📊 Performance Monitoring:\")\n",
    "    print(\"   • Processing time tracking\")\n",
    "    print(\"   • API call counting\")\n",
    "    print(\"   • Success/failure rates\")\n",
    "    \n",
    "    # Example usage\n",
    "    processor = OptimizedPDFProcessor()\n",
    "    \n",
    "    print(f\"\\\\n💾 Cache directory: {processor.cache_dir}\")\n",
    "    print(f\"📁 Cache size: {len(list(processor.cache_dir.glob('*.pkl')))} files\")\n",
    "    \n",
    "    return processor\n",
    "\n",
    "# Initialize optimized processor\n",
    "processor = demonstrate_optimization()\n",
    "\n",
    "# Example batch processing\n",
    "def run_batch_example(pdf_directory: str = \"/content/legal_pdfs/\"):\n",
    "    \"\"\"Example of batch processing multiple PDFs\"\"\"\n",
    "    \n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    if not pdf_dir.exists():\n",
    "        print(f\"❌ Directory not found: {pdf_directory}\")\n",
    "        print(\"📁 Create directory and upload PDFs to test batch processing\")\n",
    "        return\n",
    "    \n",
    "    # Find all PDFs\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"❌ No PDF files found in {pdf_directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    # Run batch processing\n",
    "    batch_results = processor.batch_process_pdfs(\n",
    "        pdf_paths=[str(p) for p in pdf_files],\n",
    "        max_workers=2,  # Conservative for API limits\n",
    "        max_pages=2,    # First 2 pages only for demo\n",
    "        preferred_model=\"auto\",\n",
    "        rate_limit_delay=0.5\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\\\n📊 BATCH RESULTS:\")\n",
    "    info = batch_results['batch_info']\n",
    "    print(f\"✅ Successful: {info['successful']}/{info['total_files']}\")\n",
    "    print(f\"⏱️  Total time: {info['total_processing_time']}s\")\n",
    "    print(f\"📈 Avg per file: {info['avg_time_per_file']}s\")\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "print(\"\\\\n🚀 Ready for optimized processing!\")\n",
    "print(\"💡 Tips for Google Colab:\")\n",
    "print(\"   • Use GPU runtime for faster processing\")\n",
    "print(\"   • Enable high-RAM if processing many files\")\n",
    "print(\"   • Monitor API usage to avoid rate limits\")\n",
    "print(\"   • Cache results to avoid reprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368b0cb",
   "metadata": {},
   "source": [
    "## 📊 Export Results and Visualization\n",
    "\n",
    "Save labeled PDF content and create insightful visualizations of the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e17ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_to_formats(results: Dict, output_dir: str = \"/content/parsed/EN\"):\n",
    "    \"\"\"Export Malaysian legal analysis results to multiple formats in parsed folder structure\"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if 'document_info' not in results:\n",
    "        logger.error(\"Invalid results format\")\n",
    "        return\n",
    "    \n",
    "    doc_info = results['document_info']\n",
    "    doc_name = doc_info['source_file'].replace('.pdf', '')\n",
    "    language = doc_info.get('language', 'unknown')\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Complete JSON export (this is the main output for parsed folder)\n",
    "    json_file = output_path / f\"{doc_name}_complete_analysis.json\"\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 2. Structured CSV for legal review and analysis\n",
    "    elements_data = []\n",
    "    for element in results['labeled_elements']:\n",
    "        ai_labels = element.get('ai_labels', {})\n",
    "        row = {\n",
    "            'document_name': doc_name,\n",
    "            'language': language,\n",
    "            'index': element['index'],\n",
    "            'page_number': element['page_number'],\n",
    "            'element_type': element['element_type'],\n",
    "            'character_count': element['character_count'],\n",
    "            'content_category': ai_labels.get('content_category', 'unknown'),\n",
    "            'legal_significance': ai_labels.get('legal_significance', 'unknown'),\n",
    "            'subject_matter': ai_labels.get('subject_matter', 'unknown'),\n",
    "            'confidence_score': ai_labels.get('confidence_score', 0),\n",
    "            'contains_definitions': ai_labels.get('contains_definitions', False),\n",
    "            'contains_penalties': ai_labels.get('contains_penalties', False),\n",
    "            'actionable_requirements': ai_labels.get('actionable_requirements', False),\n",
    "            'references_other_sections': ai_labels.get('references_other_sections', False),\n",
    "            'model_used': ai_labels.get('model_used', 'unknown'),\n",
    "            'keywords': ', '.join(ai_labels.get('keywords', [])) if ai_labels.get('keywords') else '',\n",
    "            'summary': ai_labels.get('summary', ''),\n",
    "            'text_preview': element['text'][:200] + \"...\" if len(element['text']) > 200 else element['text']\n",
    "        }\n",
    "        elements_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(elements_data)\n",
    "    csv_file = output_path / f\"{doc_name}_legal_analysis.csv\"\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # 3. Legal summary report for human review\n",
    "    report_file = output_path / f\"{doc_name}_legal_summary.txt\"\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        info = results['document_info']\n",
    "        summary = results['analysis_summary']\n",
    "        \n",
    "        f.write(f\"MALAYSIAN LEGAL DOCUMENT ANALYSIS REPORT\\\\n\")\n",
    "        f.write(f\"{'='*60}\\\\n\\\\n\")\n",
    "        f.write(f\"Document: {info['source_file']}\\\\n\")\n",
    "        f.write(f\"Language: {info.get('language', 'Unknown')}\\\\n\")\n",
    "        f.write(f\"Source Path: {info.get('source_path', 'Unknown')}\\\\n\")\n",
    "        f.write(f\"Processed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "        f.write(f\"Processing Time: {info['processing_time_seconds']}s\\\\n\")\n",
    "        f.write(f\"Extraction Method: {info.get('extraction_method', 'Unknown')}\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(f\"DOCUMENT STATISTICS:\\\\n\")\n",
    "        f.write(f\"{'-'*30}\\\\n\")\n",
    "        f.write(f\"Total Elements: {info['total_elements']}\\\\n\")\n",
    "        f.write(f\"Total Characters: {info['total_characters']:,}\\\\n\")\n",
    "        f.write(f\"API Calls Made: {info['api_calls_made']}\\\\n\")\n",
    "        f.write(f\"Models Used: {', '.join(info.get('models_used', []))}\\\\n\")\n",
    "        f.write(f\"Average Confidence: {summary['average_confidence']}\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(f\"LEGAL CONTENT ANALYSIS:\\\\n\")\n",
    "        f.write(f\"{'-'*30}\\\\n\")\n",
    "        for category, count in summary['content_categories'].items():\n",
    "            percentage = summary['category_distribution'][category]\n",
    "            f.write(f\"{category}: {count} elements ({percentage}%)\\\\n\")\n",
    "        \n",
    "        f.write(f\"\\\\nLEGAL SIGNIFICANCE:\\\\n\")\n",
    "        f.write(f\"{'-'*30}\\\\n\")\n",
    "        f.write(f\"High Importance Elements: {summary['high_importance_elements']}\\\\n\")\n",
    "        \n",
    "        # Extract high importance elements\n",
    "        high_importance_elements = [\n",
    "            el for el in results['labeled_elements'] \n",
    "            if el.get('ai_labels', {}).get('legal_significance') == 'high'\n",
    "        ]\n",
    "        \n",
    "        if high_importance_elements:\n",
    "            f.write(f\"\\\\nHIGH IMPORTANCE SECTIONS:\\\\n\")\n",
    "            f.write(f\"{'-'*30}\\\\n\")\n",
    "            for i, element in enumerate(high_importance_elements[:10]):  # Top 10\n",
    "                ai_labels = element.get('ai_labels', {})\n",
    "                f.write(f\"{i+1}. Page {element['page_number']} - {ai_labels.get('content_category', 'Unknown')}\\\\n\")\n",
    "                f.write(f\"   Subject: {ai_labels.get('subject_matter', 'Unknown')}\\\\n\")\n",
    "                f.write(f\"   Text: {element['text'][:150]}{'...' if len(element['text']) > 150 else ''}\\\\n\\\\n\")\n",
    "    \n",
    "    # 4. Create a metadata index file for the parsed folder\n",
    "    metadata_file = output_path / f\"{doc_name}_metadata.json\"\n",
    "    metadata = {\n",
    "        'document_info': doc_info,\n",
    "        'analysis_summary': summary,\n",
    "        'files_generated': {\n",
    "            'complete_analysis': json_file.name,\n",
    "            'csv_analysis': csv_file.name,\n",
    "            'text_summary': report_file.name,\n",
    "            'metadata': metadata_file.name\n",
    "        },\n",
    "        'generation_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logger.info(f\"✅ Malaysian legal analysis exported to: {output_path}\")\n",
    "    logger.info(f\"📄 Files created:\")\n",
    "    logger.info(f\"   • {json_file.name} (Complete analysis - main output)\")\n",
    "    logger.info(f\"   • {csv_file.name} (Structured data for review)\")\n",
    "    logger.info(f\"   • {report_file.name} (Human-readable summary)\")\n",
    "    logger.info(f\"   • {metadata_file.name} (Document metadata)\")\n",
    "    \n",
    "    return {\n",
    "        'json_file': str(json_file),\n",
    "        'csv_file': str(csv_file),\n",
    "        'report_file': str(report_file),\n",
    "        'metadata_file': str(metadata_file),\n",
    "        'dataframe': df,\n",
    "        'output_directory': str(output_path)\n",
    "    }\n",
    "\n",
    "def create_analysis_visualizations(results: Dict, output_dir: str = \"/content/parsed/EN\"):\n",
    "    \"\"\"Create comprehensive visualizations for Malaysian legal document analysis\"\"\"\n",
    "    \n",
    "    if 'labeled_elements' not in results:\n",
    "        logger.error(\"No labeled elements found for visualization\")\n",
    "        return\n",
    "    \n",
    "    doc_info = results['document_info']\n",
    "    doc_name = doc_info['source_file'].replace('.pdf', '')\n",
    "    language = doc_info.get('language', 'unknown')\n",
    "    \n",
    "    # Prepare data\n",
    "    elements_data = []\n",
    "    for element in results['labeled_elements']:\n",
    "        ai_labels = element.get('ai_labels', {})\n",
    "        elements_data.append({\n",
    "            'page': element['page_number'],\n",
    "            'element_type': element['element_type'],\n",
    "            'category': ai_labels.get('content_category', 'unknown'),\n",
    "            'significance': ai_labels.get('legal_significance', 'unknown'),\n",
    "            'confidence': ai_labels.get('confidence_score', 0),\n",
    "            'char_count': element['character_count'],\n",
    "            'has_definitions': ai_labels.get('contains_definitions', False),\n",
    "            'has_penalties': ai_labels.get('contains_penalties', False),\n",
    "            'has_requirements': ai_labels.get('actionable_requirements', False)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(elements_data)\n",
    "    \n",
    "    # Create enhanced visualizations for legal documents\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle(f'Malaysian Legal Document Analysis: {doc_name} ({language})', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Content Categories Distribution\n",
    "    category_counts = df['category'].value_counts()\n",
    "    colors = plt.cm.Set3(range(len(category_counts)))\n",
    "    axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', \n",
    "                  startangle=90, colors=colors)\n",
    "    axes[0, 0].set_title('Legal Content Categories')\n",
    "    \n",
    "    # 2. Legal Significance Distribution\n",
    "    significance_counts = df['significance'].value_counts()\n",
    "    colors_sig = {'high': 'red', 'medium': 'orange', 'low': 'green', 'unknown': 'gray'}\n",
    "    bar_colors = [colors_sig.get(sig, 'gray') for sig in significance_counts.index]\n",
    "    bars = axes[0, 1].bar(significance_counts.index, significance_counts.values, color=bar_colors)\n",
    "    axes[0, 1].set_title('Legal Significance Levels')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # 3. Elements Distribution by Page\n",
    "    page_counts = df['page'].value_counts().sort_index()\n",
    "    axes[0, 2].plot(page_counts.index, page_counts.values, marker='o', linewidth=2, markersize=6, color='blue')\n",
    "    axes[0, 2].set_title('Elements per Page')\n",
    "    axes[0, 2].set_xlabel('Page Number')\n",
    "    axes[0, 2].set_ylabel('Element Count')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Element Types Distribution\n",
    "    type_counts = df['element_type'].value_counts()\n",
    "    axes[1, 0].barh(type_counts.index, type_counts.values, color='skyblue')\n",
    "    axes[1, 0].set_title('Document Element Types')\n",
    "    axes[1, 0].set_xlabel('Count')\n",
    "    \n",
    "    # 5. AI Confidence Score Distribution\n",
    "    axes[1, 1].hist(df['confidence'], bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_title('AI Confidence Score Distribution')\n",
    "    axes[1, 1].set_xlabel('Confidence Score')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    mean_conf = df['confidence'].mean()\n",
    "    axes[1, 1].axvline(mean_conf, color='red', linestyle='--', label=f'Mean: {mean_conf:.2f}')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # 6. Character Count vs Confidence (colored by page)\n",
    "    scatter = axes[1, 2].scatter(df['char_count'], df['confidence'], c=df['page'], \n",
    "                               cmap='viridis', alpha=0.6, s=30)\n",
    "    axes[1, 2].set_title('Content Length vs AI Confidence')\n",
    "    axes[1, 2].set_xlabel('Character Count')\n",
    "    axes[1, 2].set_ylabel('Confidence Score')\n",
    "    plt.colorbar(scatter, ax=axes[1, 2], label='Page Number')\n",
    "    \n",
    "    # 7. Legal Features Analysis\n",
    "    features = ['has_definitions', 'has_penalties', 'has_requirements']\n",
    "    feature_counts = [df[feature].sum() for feature in features]\n",
    "    feature_labels = ['Contains Definitions', 'Contains Penalties', 'Has Requirements']\n",
    "    axes[2, 0].bar(feature_labels, feature_counts, color=['purple', 'orange', 'green'])\n",
    "    axes[2, 0].set_title('Legal Features Detection')\n",
    "    axes[2, 0].set_ylabel('Count')\n",
    "    plt.setp(axes[2, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 8. High Importance Elements by Page\n",
    "    high_importance = df[df['significance'] == 'high']\n",
    "    if not high_importance.empty:\n",
    "        high_by_page = high_importance['page'].value_counts().sort_index()\n",
    "        axes[2, 1].bar(high_by_page.index, high_by_page.values, color='red', alpha=0.7)\n",
    "        axes[2, 1].set_title('High Importance Elements by Page')\n",
    "        axes[2, 1].set_xlabel('Page Number')\n",
    "        axes[2, 1].set_ylabel('High Importance Count')\n",
    "    else:\n",
    "        axes[2, 1].text(0.5, 0.5, 'No High Importance\\\\nElements Found', \n",
    "                       ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "        axes[2, 1].set_title('High Importance Elements by Page')\n",
    "    \n",
    "    # 9. Category vs Significance Heatmap\n",
    "    if len(df) > 0:\n",
    "        pivot_table = df.pivot_table(values='confidence', index='category', \n",
    "                                   columns='significance', aggfunc='count', fill_value=0)\n",
    "        if not pivot_table.empty:\n",
    "            im = axes[2, 2].imshow(pivot_table.values, cmap='YlOrRd', aspect='auto')\n",
    "            axes[2, 2].set_xticks(range(len(pivot_table.columns)))\n",
    "            axes[2, 2].set_yticks(range(len(pivot_table.index)))\n",
    "            axes[2, 2].set_xticklabels(pivot_table.columns)\n",
    "            axes[2, 2].set_yticklabels(pivot_table.index)\n",
    "            axes[2, 2].set_title('Category vs Significance Heatmap')\n",
    "            plt.colorbar(im, ax=axes[2, 2])\n",
    "        else:\n",
    "            axes[2, 2].text(0.5, 0.5, 'Insufficient Data\\\\nfor Heatmap', \n",
    "                           ha='center', va='center', transform=axes[2, 2].transAxes)\n",
    "            axes[2, 2].set_title('Category vs Significance Heatmap')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    output_path = Path(output_dir)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    viz_file = output_path / f\"{doc_name}_legal_analysis_dashboard.png\"\n",
    "    plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"📊 Legal analysis dashboard saved: {viz_file}\")\n",
    "    \n",
    "    # Enhanced analysis summary\n",
    "    print(f\"\\\\n\udcca MALAYSIAN LEGAL DOCUMENT ANALYSIS SUMMARY:\")\n",
    "    print(f\"📄 Document: {doc_name} ({language})\")\n",
    "    print(f\"📈 Total elements: {len(df)}\")\n",
    "    print(f\"🎯 Average AI confidence: {df['confidence'].mean():.2f}\")\n",
    "    print(f\"📝 Average chars per element: {df['char_count'].mean():.0f}\")\n",
    "    print(f\"⭐ High confidence elements (>0.8): {len(df[df['confidence'] > 0.8])}\")\n",
    "    print(f\"🚨 High importance elements: {len(df[df['significance'] == 'high'])}\")\n",
    "    \n",
    "    print(f\"\\\\n🏷️  TOP LEGAL CATEGORIES:\")\n",
    "    for cat, count in category_counts.head().items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   • {cat}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\n⚖️  LEGAL FEATURES DETECTED:\")\n",
    "    print(f\"   • Documents with definitions: {df['has_definitions'].sum()}\")\n",
    "    print(f\"   • Documents with penalties: {df['has_penalties'].sum()}\")\n",
    "    print(f\"   • Documents with requirements: {df['has_requirements'].sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def demonstrate_malaysian_legal_export():\n",
    "    \"\"\"Demonstrate export workflow for Malaysian legal documents\"\"\"\n",
    "    \n",
    "    print(\"📊 MALAYSIAN LEGAL DOCUMENT EXPORT WORKFLOW:\")\n",
    "    print(\"\\\\n1. 📄 Structured Output for Parsed Folder:\")\n",
    "    print(\"   • Complete JSON analysis (main output)\")\n",
    "    print(\"   • CSV file for legal review and analysis\")\n",
    "    print(\"   • Human-readable summary report\")\n",
    "    print(\"   • Metadata index for document tracking\")\n",
    "    \n",
    "    print(\"\\\\n2. 📈 Legal-Specific Visualizations:\")\n",
    "    print(\"   • Content category analysis\")\n",
    "    print(\"   • Legal significance assessment\")\n",
    "    print(\"   • Feature detection (definitions, penalties, requirements)\")\n",
    "    print(\"   • Page-by-page importance mapping\")\n",
    "    print(\"   • AI confidence and quality metrics\")\n",
    "    \n",
    "    print(\"\\\\n3. 🎯 Legal Analysis Features:\")\n",
    "    print(\"   • High importance section identification\")\n",
    "    print(\"   • Legal terminology detection\")\n",
    "    print(\"   • Cross-reference analysis\")\n",
    "    print(\"   • Language-aware processing\")\n",
    "    \n",
    "    print(\"\\\\n💡 Usage for Malaysian Legal Acts:\")\n",
    "    print(\"   # Process single document\")\n",
    "    print(\"   results = process_pdf_with_ai_labeling('malaysian_acts/EN/act125.pdf')\")\n",
    "    print(\"   export_data = export_results_to_formats(results, 'parsed/EN')\")\n",
    "    print(\"   df = create_analysis_visualizations(results, 'parsed/EN')\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "demonstrate_malaysian_legal_export()\n",
    "\n",
    "print(\"\\\\n✅ Malaysian legal document export functions ready!\")\n",
    "print(\"\udfdb️  Optimized for legal document analysis and parsed folder structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92442af4",
   "metadata": {},
   "source": [
    "## 🚀 Complete Processing Workflow\n",
    "\n",
    "Now we'll put everything together into a complete workflow that processes your Malaysian legal PDFs with AI-powered content labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def complete_malaysian_legal_analysis(\n",
    "    pdf_path: str, \n",
    "    use_openai: bool = True,\n",
    "    use_gemini: bool = False,\n",
    "    output_dir: str = \"/content/parsed\",\n",
    "    language: str = \"auto\"  # \"EN\", \"BM\", or \"auto\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete workflow for analyzing Malaysian legal PDFs with AI labeling\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file (from malaysian_acts folder)\n",
    "        use_openai: Whether to use OpenAI GPT-4 for labeling\n",
    "        use_gemini: Whether to use Google Gemini for labeling\n",
    "        output_dir: Directory to save results (parsed folder)\n",
    "        language: Document language (\"EN\", \"BM\", or \"auto\" for detection)\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing complete analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"🏛️  Starting Malaysian Legal PDF Analysis: {Path(pdf_path).name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Detect language if auto\n",
    "        if language == \"auto\":\n",
    "            if '/EN/' in pdf_path.upper() or '/en/' in pdf_path.lower():\n",
    "                language = \"EN\"\n",
    "            elif '/BM/' in pdf_path.upper() or '/bm/' in pdf_path.lower():\n",
    "                language = \"BM\"\n",
    "            else:\n",
    "                language = \"other\"\n",
    "        \n",
    "        print(f\"🌐 Document language: {language}\")\n",
    "        \n",
    "        # Step 2: Extract PDF content with unstructured\n",
    "        print(\"📄 Step 1: Extracting PDF content...\")\n",
    "        elements = extract_pdf_content(pdf_path)\n",
    "        if not elements:\n",
    "            raise ValueError(\"No elements extracted from PDF\")\n",
    "        \n",
    "        print(f\"✅ Extracted {len(elements)} elements\")\n",
    "        \n",
    "        # Step 3: Set up AI models\n",
    "        print(\"🤖 Step 2: Setting up AI models...\")\n",
    "        \n",
    "        clients = {}\n",
    "        if use_openai and ai_system.openai_client:\n",
    "            clients['openai'] = ai_system.openai_client\n",
    "            print(\"✅ OpenAI GPT-4 ready\")\n",
    "        \n",
    "        if use_gemini and ai_system.gemini_model:\n",
    "            clients['gemini'] = ai_system.gemini_model\n",
    "            print(\"✅ Google Gemini ready\")\n",
    "        \n",
    "        if not clients:\n",
    "            raise ValueError(\"No AI models available. Please configure API keys.\")\n",
    "        \n",
    "        # Step 4: Process elements with AI labeling\n",
    "        print(\"🏷️  Step 3: AI content labeling...\")\n",
    "        \n",
    "        labeled_elements = []\n",
    "        total_api_calls = 0\n",
    "        \n",
    "        for i, element in enumerate(elements):\n",
    "            print(f\"Processing element {i+1}/{len(elements)} (Page {element['page_number']})...\", end=\"\\\\r\")\n",
    "            \n",
    "            # Choose AI model (prefer OpenAI for legal documents)\n",
    "            if 'openai' in clients:\n",
    "                ai_labels = smart_label_content(ai_system, element['text'], element['element_type'], \"openai\")\n",
    "                if ai_labels and 'error' not in ai_labels:\n",
    "                    ai_labels['model_used'] = 'gpt-4'\n",
    "                    total_api_calls += 1\n",
    "            elif 'gemini' in clients:\n",
    "                ai_labels = smart_label_content(ai_system, element['text'], element['element_type'], \"gemini\")\n",
    "                if ai_labels and 'error' not in ai_labels:\n",
    "                    ai_labels['model_used'] = 'gemini-pro'\n",
    "                    total_api_calls += 1\n",
    "            else:\n",
    "                ai_labels = {}\n",
    "            \n",
    "            element['ai_labels'] = ai_labels\n",
    "            labeled_elements.append(element)\n",
    "            \n",
    "            # Rate limiting\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                time.sleep(1)  # Brief pause every 10 elements\n",
    "        \n",
    "        print(f\"\\\\n✅ Completed AI labeling with {total_api_calls} API calls\")\n",
    "        \n",
    "        # Step 5: Create analysis summary\n",
    "        print(\"📊 Step 4: Generating analysis summary...\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_chars = sum(len(el['text']) for el in labeled_elements)\n",
    "        content_categories = {}\n",
    "        high_importance = 0\n",
    "        category_distribution = {}\n",
    "        \n",
    "        for element in labeled_elements:\n",
    "            ai_labels = element.get('ai_labels', {})\n",
    "            category = ai_labels.get('content_category', 'unknown')\n",
    "            significance = ai_labels.get('legal_significance', 'unknown')\n",
    "            \n",
    "            content_categories[category] = content_categories.get(category, 0) + 1\n",
    "            \n",
    "            if significance == 'high':\n",
    "                high_importance += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_elements = len(labeled_elements)\n",
    "        for category, count in content_categories.items():\n",
    "            category_distribution[category] = round((count / total_elements) * 100, 1)\n",
    "        \n",
    "        # Step 6: Compile complete results\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        results = {\n",
    "            'document_info': {\n",
    "                'source_file': Path(pdf_path).name,\n",
    "                'source_path': str(pdf_path),\n",
    "                'language': language,\n",
    "                'total_elements': total_elements,\n",
    "                'total_characters': total_chars,\n",
    "                'processing_time_seconds': round(processing_time, 2),\n",
    "                'api_calls_made': total_api_calls,\n",
    "                'models_used': list(clients.keys()),\n",
    "                'processed_at': datetime.now().isoformat(),\n",
    "                'extraction_method': 'unstructured_hi_res'\n",
    "            },\n",
    "            'labeled_elements': labeled_elements,\n",
    "            'analysis_summary': {\n",
    "                'content_categories': content_categories,\n",
    "                'category_distribution': category_distribution,\n",
    "                'high_importance_elements': high_importance,\n",
    "                'average_confidence': round(\n",
    "                    sum(el.get('ai_labels', {}).get('confidence_score', 0) for el in labeled_elements) / total_elements, 3\n",
    "                ) if total_elements > 0 else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Step 7: Save to parsed folder with proper structure\n",
    "        print(\"💾 Step 5: Saving to parsed folder...\")\n",
    "        \n",
    "        # Create output directory structure\n",
    "        parsed_path = Path(output_dir)\n",
    "        lang_output_dir = parsed_path / language\n",
    "        lang_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save complete JSON analysis\n",
    "        output_filename = f\"{Path(pdf_path).stem}_analysis.json\"\n",
    "        output_file = lang_output_dir / output_filename\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"✅ Analysis saved: {output_file}\")\n",
    "        \n",
    "        # Step 8: Export additional formats\n",
    "        print(\"📊 Step 6: Creating additional exports...\")\n",
    "        export_data = export_results_to_formats(results, str(lang_output_dir))\n",
    "        \n",
    "        # Step 9: Create visualizations\n",
    "        print(\"📈 Step 7: Creating visualizations...\")\n",
    "        df = create_analysis_visualizations(results, str(lang_output_dir))\n",
    "        \n",
    "        logger.info(f\"🎉 Analysis completed in {processing_time:.2f} seconds\")\n",
    "        logger.info(f\"📊 Summary: {total_elements} elements, {high_importance} high importance\")\n",
    "        logger.info(f\"📁 Saved to: {output_file}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Analysis failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Updated demo function for Malaysian legal documents\n",
    "def quick_malaysian_legal_demo():\n",
    "    \"\"\"Demonstrate the analysis workflow specifically for Malaysian legal documents\"\"\"\n",
    "    \n",
    "    print(\"\udfdb️  MALAYSIAN LEGAL PDF AI ANALYSIS SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\\\n📋 Specialized Features for Malaysian Legal Documents:\")\n",
    "    print(\"• 📄 PDF extraction optimized for legal formatting\")\n",
    "    print(\"• 🤖 AI models trained to understand legal terminology\")\n",
    "    print(\"• 🏛️  Malaysian legal document structure recognition\")\n",
    "    print(\"• 🌐 Language-aware processing (English/Bahasa Malaysia)\")\n",
    "    print(\"• 📊 Legal significance classification\")\n",
    "    print(\"• 💾 Structured output in JSON format for parsed folder\")\n",
    "    print(\"• ⚡ Batch processing for multiple acts\")\n",
    "    \n",
    "    print(\"\\\\n📂 Folder Structure:\")\n",
    "    print(\"   📂 malaysian_acts/ (Input)\")\n",
    "    print(\"      📂 EN/ (English legal documents)\")\n",
    "    print(\"      📂 BM/ (Bahasa Malaysia legal documents)\")\n",
    "    print(\"   📂 parsed/ (Output)\")\n",
    "    print(\"      📂 EN/ (English analysis results)\")\n",
    "    print(\"      📂 BM/ (Bahasa Malaysia analysis results)\")\n",
    "    \n",
    "    print(\"\\\\n🚀 Quick Start for Malaysian Legal Acts:\")\n",
    "    print(\"1. Ensure your malaysian_acts folder is uploaded to Colab\")\n",
    "    print(\"2. Configure API keys (OpenAI or Gemini)\")\n",
    "    print(\"3. Run batch processing or individual file analysis\")\n",
    "    print(\"4. Review results in the parsed folder\")\n",
    "    \n",
    "    print(\"\\\\n💡 API Key Setup (Colab Secrets):\")\n",
    "    print(\"• In Colab: Go to 🔑 Secrets panel\")\n",
    "    print(\"• Add OPENAI_API_KEY and/or GEMINI_API_KEY\")\n",
    "    print(\"• Enable 'Notebook access' for each key\")\n",
    "    \n",
    "    print(\"\\\\n⚠️  Processing Notes:\")\n",
    "    print(\"• Legal documents are processed with high-resolution extraction\")\n",
    "    print(\"• AI models provide specialized legal classification\")\n",
    "    print(\"• Results include legal significance ratings\")\n",
    "    print(\"• Processing time varies with document complexity\")\n",
    "    print(\"• API costs apply for AI analysis\")\n",
    "    \n",
    "    print(\"\\\\n🎯 Legal Classification Categories:\")\n",
    "    print(\"• section_header, legal_definition, procedural_requirement\")\n",
    "    print(\"• penalty_clause, schedule, interpretation, preamble\")\n",
    "    print(\"• Legal significance: high, medium, low\")\n",
    "    print(\"• Subject matter: criminal_procedure, corporate_law, etc.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "quick_malaysian_legal_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c4337",
   "metadata": {},
   "source": [
    "## 🧪 Testing Your Malaysian Legal PDFs\n",
    "\n",
    "Ready to test the system! Upload your Malaysian legal PDF files and run the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 STEP 1: Setup Malaysian Legal PDFs from Google Drive\n",
    "# Complete setup for accessing your PDF files in Google Colab\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from google.colab import drive, files\n",
    "\n",
    "def setup_malaysian_legal_pdfs_from_drive():\n",
    "    \"\"\"Complete setup for Malaysian legal PDFs from Google Drive in Google Colab\"\"\"\n",
    "    \n",
    "    print(\"🏛️  MALAYSIAN LEGAL PDF SETUP FOR GOOGLE COLAB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Check if files already exist in Colab workspace\n",
    "    colab_dir = \"/content/malaysian_acts\"\n",
    "    if os.path.exists(colab_dir):\n",
    "        print(\"✅ malaysian_acts folder already exists in Colab\")\n",
    "        check_folder_contents(colab_dir)\n",
    "        return colab_dir\n",
    "    \n",
    "    # Step 2: Mount Google Drive\n",
    "    print(\"🔄 Mounting Google Drive...\")\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"✅ Google Drive mounted successfully\")\n",
    "        \n",
    "        # Step 3: Look for malaysian_acts folder in common Drive locations\n",
    "        print(\"🔍 Searching for malaysian_acts folder in Google Drive...\")\n",
    "        drive_search_paths = [\n",
    "            \"/content/drive/MyDrive/malaysian_acts\",\n",
    "            \"/content/drive/MyDrive/reg-intel/malaysian_acts\",\n",
    "            \"/content/drive/MyDrive/Github/reg-intel/malaysian_acts\",\n",
    "            \"/content/drive/MyDrive/Downloads/malaysian_acts\",\n",
    "            \"/content/drive/MyDrive/Documents/malaysian_acts\"\n",
    "        ]\n",
    "        \n",
    "        source_dir = None\n",
    "        for path in drive_search_paths:\n",
    "            if os.path.exists(path):\n",
    "                source_dir = path\n",
    "                print(f\"✅ Found malaysian_acts folder at: {source_dir}\")\n",
    "                break\n",
    "        \n",
    "        if source_dir:\n",
    "            # Step 4: Copy from Drive to Colab workspace for faster processing\n",
    "            print(\"🔄 Copying malaysian_acts folder to Colab workspace...\")\n",
    "            shutil.copytree(source_dir, colab_dir)\n",
    "            print(\"✅ Successfully copied malaysian_acts folder to /content/\")\n",
    "            \n",
    "            # Verify the copy\n",
    "            check_folder_contents(colab_dir)\n",
    "            return colab_dir\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ malaysian_acts folder not found in Google Drive\")\n",
    "            print(\"\\n📤 UPLOAD INSTRUCTIONS:\")\n",
    "            print(\"1. Go to your Google Drive (drive.google.com)\")\n",
    "            print(\"2. Upload your 'malaysian_acts' folder with EN/ and BM/ subfolders\")\n",
    "            print(\"3. Recommended location: MyDrive/malaysian_acts/\")\n",
    "            print(\"4. Re-run this cell after upload\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Google Drive mount failed: {e}\")\n",
    "        print(\"💡 Alternative: Use manual file upload\")\n",
    "        return None\n",
    "\n",
    "def check_folder_contents(base_dir):\n",
    "    \"\"\"Check and display Malaysian legal PDFs folder contents\"\"\"\n",
    "    print(f\"\\n📂 MALAYSIAN LEGAL ACTS FOLDER: {base_dir}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    base_path = Path(base_dir)\n",
    "    if not base_path.exists():\n",
    "        print(\"❌ Folder does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Check EN (English) folder\n",
    "    en_dir = base_path / \"EN\"\n",
    "    if en_dir.exists():\n",
    "        en_files = list(en_dir.glob(\"*.pdf\"))\n",
    "        print(f\"📂 EN/ (English): {len(en_files)} PDF files\")\n",
    "        for i, file in enumerate(en_files[:5]):  # Show first 5\n",
    "            print(f\"   📄 {file.name}\")\n",
    "        if len(en_files) > 5:\n",
    "            print(f\"   ... and {len(en_files) - 5} more files\")\n",
    "    else:\n",
    "        print(\"❌ EN/ folder not found\")\n",
    "    \n",
    "    # Check BM (Bahasa Malaysia) folder\n",
    "    bm_dir = base_path / \"BM\"\n",
    "    if bm_dir.exists():\n",
    "        bm_files = list(bm_dir.glob(\"*.pdf\"))\n",
    "        print(f\"📂 BM/ (Bahasa Malaysia): {len(bm_files)} PDF files\")\n",
    "        for i, file in enumerate(bm_files[:5]):  # Show first 5\n",
    "            print(f\"   📄 {file.name}\")\n",
    "        if len(bm_files) > 5:\n",
    "            print(f\"   ... and {len(bm_files) - 5} more files\")\n",
    "    else:\n",
    "        print(\"❌ BM/ folder not found\")\n",
    "    \n",
    "    # Summary\n",
    "    total_files = len(list(base_path.glob(\"**/*.pdf\")))\n",
    "    print(f\"\\n📊 Total PDF files found: {total_files}\")\n",
    "\n",
    "def setup_output_directories():\n",
    "    \"\"\"Setup output directories for parsed results\"\"\"\n",
    "    \n",
    "    # Create local output directory\n",
    "    local_output = Path(\"/content/parsed\")\n",
    "    local_output.mkdir(exist_ok=True)\n",
    "    (local_output / \"EN\").mkdir(exist_ok=True)\n",
    "    (local_output / \"BM\").mkdir(exist_ok=True)\n",
    "    (local_output / \"other\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create Drive backup directory (if Drive is mounted)\n",
    "    if os.path.exists(\"/content/drive/MyDrive\"):\n",
    "        drive_output = Path(\"/content/drive/MyDrive/parsed\")\n",
    "        drive_output.mkdir(exist_ok=True)\n",
    "        (drive_output / \"EN\").mkdir(exist_ok=True)\n",
    "        (drive_output / \"BM\").mkdir(exist_ok=True)\n",
    "        (drive_output / \"other\").mkdir(exist_ok=True)\n",
    "        print(\"✅ Output directories created:\")\n",
    "        print(f\"   📁 Local: {local_output}\")\n",
    "        print(f\"   📁 Drive backup: {drive_output}\")\n",
    "    else:\n",
    "        print(\"✅ Local output directory created:\")\n",
    "        print(f\"   📁 Local: {local_output}\")\n",
    "\n",
    "def manual_upload_alternative():\n",
    "    \"\"\"Alternative manual upload method\"\"\"\n",
    "    print(\"\\n📤 ALTERNATIVE: MANUAL FILE UPLOAD\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"If Google Drive setup doesn't work, you can upload files manually:\")\n",
    "    print()\n",
    "    print(\"1. Run this code to upload files:\")\n",
    "    print(\"   uploaded = files.upload()\")\n",
    "    print()\n",
    "    print(\"2. Or use the file browser:\")\n",
    "    print(\"   • Click 📁 folder icon in left sidebar\")\n",
    "    print(\"   • Create folder structure: malaysian_acts/EN/ and malaysian_acts/BM/\")\n",
    "    print(\"   • Upload PDF files to appropriate folders\")\n",
    "    print()\n",
    "    print(\"3. After upload, verify with:\")\n",
    "    print(\"   check_folder_contents('/content/malaysian_acts')\")\n",
    "\n",
    "# 🚀 MAIN SETUP EXECUTION\n",
    "print(\"🇲🇾 STARTING MALAYSIAN LEGAL PDF SETUP...\")\n",
    "pdf_directory = setup_malaysian_legal_pdfs_from_drive()\n",
    "\n",
    "if pdf_directory:\n",
    "    print(f\"\\n✅ SUCCESS: PDFs ready at {pdf_directory}\")\n",
    "    \n",
    "    # Setup output directories\n",
    "    setup_output_directories()\n",
    "    \n",
    "    print(f\"\\n🎯 READY FOR ANALYSIS!\")\n",
    "    print(f\"You can now run:\")\n",
    "    print(f\"   results = await process_malaysian_acts_folder()\")\n",
    "    print(f\"   # or for specific language:\")\n",
    "    print(f\"   results = await process_malaysian_acts_folder(language_filter='EN')\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n❌ SETUP INCOMPLETE\")\n",
    "    manual_upload_alternative()\n",
    "\n",
    "# 🎯 STEP 2: Configure AI Analysis for Malaysian Legal Documents\n",
    "def configure_colab_analysis():\n",
    "    \"\"\"Configure analysis specifically for Google Colab environment\"\"\"\n",
    "    \n",
    "    print(\"\\n🤖 AI MODEL CONFIGURATION FOR GOOGLE COLAB:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check API key availability in Colab Secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        \n",
    "        # Try to get API keys from Colab Secrets\n",
    "        try:\n",
    "            openai_key = userdata.get('OPENAI_API_KEY')\n",
    "            openai_available = bool(openai_key)\n",
    "        except:\n",
    "            openai_available = False\n",
    "            \n",
    "        try:\n",
    "            gemini_key = userdata.get('GEMINI_API_KEY')\n",
    "            gemini_available = bool(gemini_key)\n",
    "        except:\n",
    "            gemini_available = False\n",
    "            \n",
    "        print(f\"🔑 API Key Status (Colab Secrets):\")\n",
    "        print(f\"   OpenAI: {'✅ Available' if openai_available else '❌ Not configured'}\")\n",
    "        print(f\"   Gemini: {'✅ Available' if gemini_available else '❌ Not configured'}\")\n",
    "        \n",
    "        if not openai_available and not gemini_available:\n",
    "            print(f\"\\n⚠️  NO API KEYS CONFIGURED!\")\n",
    "            print(f\"To add API keys in Google Colab:\")\n",
    "            print(f\"1. Click 🔑 'Secrets' in the left sidebar\")\n",
    "            print(f\"2. Add new secret:\")\n",
    "            print(f\"   • Name: OPENAI_API_KEY\")\n",
    "            print(f\"   • Value: your OpenAI API key\")\n",
    "            print(f\"   • Enable 'Notebook access'\")\n",
    "            print(f\"3. Optional: Add GEMINI_API_KEY the same way\")\n",
    "            print(f\"4. Restart runtime: Runtime → Restart runtime\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"\\n✅ AI models configured!\")\n",
    "            return True\n",
    "            \n",
    "    except ImportError:\n",
    "        print(f\"❌ Not running in Google Colab\")\n",
    "        print(f\"💡 For local development, set environment variables:\")\n",
    "        print(f\"   export OPENAI_API_KEY='your-key-here'\")\n",
    "        return False\n",
    "\n",
    "# Configure analysis\n",
    "analysis_ready = configure_colab_analysis()\n",
    "\n",
    "# 🚀 STEP 3: Quick Analysis Function for Google Colab\n",
    "async def run_colab_legal_analysis(\n",
    "    language_filter: str = \"EN\",  # \"EN\", \"BM\", or \"all\"\n",
    "    max_files: int = 3,           # Limit for testing\n",
    "    max_pages_per_file: int = 5   # Limit pages for faster testing\n",
    "):\n",
    "    \"\"\"Quick analysis function optimized for Google Colab\"\"\"\n",
    "    \n",
    "    print(\"🏛️  MALAYSIAN LEGAL PDF ANALYSIS - GOOGLE COLAB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if setup is complete\n",
    "    acts_dir = \"/content/malaysian_acts\"\n",
    "    if not os.path.exists(acts_dir):\n",
    "        print(\"❌ PDF files not found. Run the setup cell first!\")\n",
    "        return None\n",
    "    \n",
    "    if not analysis_ready:\n",
    "        print(\"❌ API keys not configured. Set up Colab Secrets first!\")\n",
    "        return None\n",
    "    \n",
    "    # Run the analysis\n",
    "    print(f\"🎯 Analysis Configuration:\")\n",
    "    print(f\"   Language filter: {language_filter}\")\n",
    "    print(f\"   Max files: {max_files}\")\n",
    "    print(f\"   Max pages per file: {max_pages_per_file}\")\n",
    "    print(f\"   Input: {acts_dir}\")\n",
    "    print(f\"   Output: /content/parsed\")\n",
    "    \n",
    "    # Execute the batch processing\n",
    "    results = await process_malaysian_acts_folder(\n",
    "        acts_dir=acts_dir,\n",
    "        output_dir=\"/content/parsed\",\n",
    "        language_filter=language_filter,\n",
    "        max_files=max_files,\n",
    "        max_pages_per_file=max_pages_per_file\n",
    "    )\n",
    "    \n",
    "    # Backup results to Drive if available\n",
    "    if os.path.exists(\"/content/drive/MyDrive\"):\n",
    "        try:\n",
    "            drive_backup = \"/content/drive/MyDrive/parsed\"\n",
    "            if os.path.exists(\"/content/parsed\"):\n",
    "                print(\"💾 Backing up results to Google Drive...\")\n",
    "                if os.path.exists(drive_backup):\n",
    "                    shutil.rmtree(drive_backup)\n",
    "                shutil.copytree(\"/content/parsed\", drive_backup)\n",
    "                print(\"✅ Results backed up to Google Drive\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Drive backup failed: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 GOOGLE COLAB SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if pdf_directory and analysis_ready:\n",
    "    print(\"\\n⚡ Ready to analyze! Run:\")\n",
    "    print(\"   results = await run_colab_legal_analysis()\")\n",
    "    print(\"\\n🎛️  Or with custom settings:\")\n",
    "    print(\"   results = await run_colab_legal_analysis(\")\n",
    "    print(\"       language_filter='EN',\")\n",
    "    print(\"       max_files=5,\")\n",
    "    print(\"       max_pages_per_file=10\")\n",
    "    print(\"   )\")\n",
    "else:\n",
    "    print(\"\\n📋 Complete the setup steps above before running analysis\")\n",
    "\n",
    "print(\"\\n💡 Google Colab Tips:\")\n",
    "print(\"• Use GPU runtime: Runtime → Change runtime type → GPU\")\n",
    "print(\"• Files in /content/ are temporary - backup important results to Drive\")\n",
    "print(\"• Colab sessions timeout after ~12 hours of inactivity\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
