{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b975526",
   "metadata": {},
   "source": [
    "# AI-Powered Malaysian Legal PDF Analysis\n",
    "## Extract PDF Content with Unstructured + Label Context with OpenAI/Gemini\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Extract text from Malaysian legal PDFs using the `unstructured` library\n",
    "2. Use AI models (OpenAI GPT-4 or Google Gemini) to intelligently label and categorize content\n",
    "3. Optimize for Google Colab GPU environment\n",
    "4. Export structured, labeled results for legal AI/RAG systems\n",
    "\n",
    "**Perfect for:** Legal document processing, content classification, and building intelligent legal knowledge bases\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ **Optimized for Google Colab GPU Runtime**\n",
    "- Make sure to enable GPU: Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be6a4d",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Install Required Libraries\n",
    "\n",
    "Install all necessary packages for PDF processing and AI model integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "!pip install -q unstructured[pdf] \n",
    "!pip install -q openai>=1.0.0\n",
    "!pip install -q google-generativeai\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q tqdm\n",
    "!pip install -q pandas\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "\n",
    "# Install additional dependencies for unstructured PDF processing\n",
    "!apt-get update -qq\n",
    "!apt-get install -qq poppler-utils tesseract-ocr\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3693c",
   "metadata": {},
   "source": [
    "## ðŸ”§ Import Dependencies and Setup API Keys\n",
    "\n",
    "Import all necessary libraries and configure secure API key handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# PDF Processing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# AI Model Libraries\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "# API Key Setup\n",
    "print(\"\\nðŸ”‘ API Key Configuration:\")\n",
    "print(\"Please set your API keys using one of these methods:\")\n",
    "print(\"1. Use Colab Secrets (recommended)\")\n",
    "print(\"2. Set environment variables\")\n",
    "print(\"3. Direct assignment (not recommended for production)\")\n",
    "\n",
    "# Secure API key handling for Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    print(\"âœ… Using Colab secrets for API keys\")\n",
    "except:\n",
    "    # Fallback to environment variables or manual input\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "    \n",
    "    if not OPENAI_API_KEY and not GEMINI_API_KEY:\n",
    "        print(\"âš ï¸  No API keys found. Please set them manually:\")\n",
    "        print(\"OPENAI_API_KEY = 'your-openai-key-here'\")\n",
    "        print(\"GEMINI_API_KEY = 'your-gemini-key-here'\")\n",
    "\n",
    "# Set API keys\n",
    "if OPENAI_API_KEY:\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    print(\"âœ… OpenAI API key configured\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"âœ… Gemini API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee1d42",
   "metadata": {},
   "source": [
    "## ðŸš€ Configure GPU and Environment\n",
    "\n",
    "Check GPU availability and optimize environment for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"âœ… GPU Available: {gpu_name}\")\n",
    "        print(f\"ðŸ“Š GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        # Set optimal settings for GPU\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        print(\"âš ï¸  No GPU detected. Using CPU (will be slower)\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  PyTorch not available. Install if you need GPU acceleration.\")\n",
    "\n",
    "# Check system resources\n",
    "import psutil\n",
    "cpu_count = psutil.cpu_count()\n",
    "memory_gb = psutil.virtual_memory().total / 1024**3\n",
    "\n",
    "print(f\"ðŸ–¥ï¸  CPU cores: {cpu_count}\")\n",
    "print(f\"ðŸ’¾ RAM: {memory_gb:.1f} GB\")\n",
    "\n",
    "# Optimize environment settings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid tokenizer warnings\n",
    "os.environ['OMP_NUM_THREADS'] = str(min(4, cpu_count))  # Optimize CPU usage\n",
    "\n",
    "print(\"âœ… Environment optimized for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6e8f3",
   "metadata": {},
   "source": [
    "## ðŸ“„ Load and Extract PDF Content\n",
    "\n",
    "Use unstructured library to extract elements from Malaysian legal PDFs with high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_path: str, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract content from PDF using unstructured library with high accuracy settings.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        max_pages: Maximum number of pages to process (None for all pages)\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted elements with metadata\n",
    "    \"\"\"\n",
    "    logger.info(f\"ðŸ” Extracting content from: {Path(pdf_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        # Configure extraction settings for maximum accuracy\n",
    "        elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            strategy=\"hi_res\",  # High resolution for legal documents\n",
    "            infer_table_structure=True,  # Detect tables\n",
    "            extract_images_in_pdf=False,  # Skip images for text focus\n",
    "            include_page_breaks=True,  # Preserve page structure\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"âœ… Extracted {len(elements)} elements from entire PDF\")\n",
    "        \n",
    "        # Convert to structured format\n",
    "        structured_elements = []\n",
    "        \n",
    "        for i, element in enumerate(elements):\n",
    "            # Get element metadata\n",
    "            page_num = 1  # default\n",
    "            coordinates = None\n",
    "            \n",
    "            if hasattr(element, 'metadata') and element.metadata:\n",
    "                if hasattr(element.metadata, 'page_number'):\n",
    "                    page_num = element.metadata.page_number\n",
    "                if hasattr(element.metadata, 'coordinates'):\n",
    "                    coordinates = str(element.metadata.coordinates)\n",
    "            \n",
    "            # Filter by page limit if specified\n",
    "            if max_pages and page_num > max_pages:\n",
    "                continue\n",
    "            \n",
    "            # Create structured element\n",
    "            structured_element = {\n",
    "                'index': i,\n",
    "                'page_number': page_num,\n",
    "                'element_type': str(type(element).__name__),\n",
    "                'text': str(element).strip(),\n",
    "                'character_count': len(str(element)),\n",
    "                'coordinates': coordinates,\n",
    "                'metadata': {\n",
    "                    'extraction_timestamp': datetime.now().isoformat(),\n",
    "                    'source_file': Path(pdf_path).name\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            structured_elements.append(structured_element)\n",
    "        \n",
    "        # Filter and sort\n",
    "        filtered_elements = [e for e in structured_elements if e['character_count'] > 5]  # Remove tiny elements\n",
    "        filtered_elements.sort(key=lambda x: (x['page_number'], x['index']))  # Sort by page and order\n",
    "        \n",
    "        logger.info(f\"ðŸ“Š Final elements: {len(filtered_elements)} (after filtering)\")\n",
    "        return filtered_elements\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error extracting PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage - Upload your PDF to Colab or use a sample\n",
    "print(\"ðŸ“ To use this function:\")\n",
    "print(\"1. Upload your Malaysian legal PDF to Colab\")\n",
    "print(\"2. Update the pdf_path variable below\")\n",
    "print(\"3. Run the extraction\")\n",
    "\n",
    "# Sample extraction (update this path)\n",
    "pdf_path = \"/content/sample_legal_document.pdf\"  # Update this path\n",
    "max_pages = 3  # Process only first 3 pages for testing\n",
    "\n",
    "print(f\"\\\\nðŸ”„ Ready to extract from: {pdf_path}\")\n",
    "print(f\"ðŸ“„ Max pages: {max_pages}\")\n",
    "\n",
    "# Uncomment the next line when you have a PDF ready\n",
    "# elements = extract_pdf_content(pdf_path, max_pages=max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f997fd1",
   "metadata": {},
   "source": [
    "## ðŸ¤– Setup AI Model Clients\n",
    "\n",
    "Initialize OpenAI GPT and Google Gemini clients for intelligent content labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AILabelingSystem:\n",
    "    \"\"\"Unified system for labeling PDF content using OpenAI or Gemini\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openai_client = None\n",
    "        self.gemini_model = None\n",
    "        self.setup_clients()\n",
    "    \n",
    "    def setup_clients(self):\n",
    "        \"\"\"Initialize available AI model clients\"\"\"\n",
    "        \n",
    "        # Setup OpenAI client\n",
    "        if OPENAI_API_KEY:\n",
    "            try:\n",
    "                self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "                logger.info(\"âœ… OpenAI client initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ OpenAI setup failed: {e}\")\n",
    "        \n",
    "        # Setup Gemini client\n",
    "        if GEMINI_API_KEY:\n",
    "            try:\n",
    "                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "                logger.info(\"âœ… Gemini client initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Gemini setup failed: {e}\")\n",
    "        \n",
    "        if not self.openai_client and not self.gemini_model:\n",
    "            logger.warning(\"âš ï¸  No AI models available. Please configure API keys.\")\n",
    "    \n",
    "    def test_models(self):\n",
    "        \"\"\"Test both models with a simple query\"\"\"\n",
    "        test_text = \"This is a test of the legal document analysis system.\"\n",
    "        \n",
    "        print(\"ðŸ§ª Testing AI Models:\\\\n\")\n",
    "        \n",
    "        # Test OpenAI\n",
    "        if self.openai_client:\n",
    "            try:\n",
    "                response = self.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": f\"Classify this text: {test_text}\"}\n",
    "                    ],\n",
    "                    max_tokens=50\n",
    "                )\n",
    "                print(\"âœ… OpenAI GPT: Working\")\n",
    "                print(f\"   Response: {response.choices[0].message.content.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ OpenAI test failed: {e}\")\n",
    "        \n",
    "        # Test Gemini\n",
    "        if self.gemini_model:\n",
    "            try:\n",
    "                response = self.gemini_model.generate_content(\n",
    "                    f\"Classify this text in one sentence: {test_text}\"\n",
    "                )\n",
    "                print(\"âœ… Gemini: Working\")\n",
    "                print(f\"   Response: {response.text.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Gemini test failed: {e}\")\n",
    "\n",
    "# Initialize the AI labeling system\n",
    "ai_system = AILabelingSystem()\n",
    "\n",
    "# Test the models\n",
    "ai_system.test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fb8a2",
   "metadata": {},
   "source": [
    "## ðŸ·ï¸ Create Context Labeling Functions\n",
    "\n",
    "Define intelligent functions to classify and label legal document content using AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeling_prompt(text: str, element_type: str) -> str:\n",
    "    \"\"\"Create a sophisticated prompt for legal content classification\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert legal document analyst specializing in Malaysian law. \n",
    "Analyze the following text from a legal document and provide a comprehensive classification.\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "\"{text}\"\n",
    "\n",
    "ELEMENT TYPE: {element_type}\n",
    "\n",
    "Please provide a JSON response with the following fields:\n",
    "1. \"content_category\": Main category (e.g., \"section_header\", \"legal_definition\", \"procedural_requirement\", \"penalty_clause\", \"table_of_contents\", \"preamble\", \"schedule\", \"interpretation\")\n",
    "2. \"legal_significance\": Level of legal importance (e.g., \"high\", \"medium\", \"low\")\n",
    "3. \"subject_matter\": What the text is about (e.g., \"criminal_procedure\", \"corporate_law\", \"tax_provisions\", \"regulatory_compliance\")\n",
    "4. \"contains_definitions\": Whether it contains legal definitions (true/false)\n",
    "5. \"contains_penalties\": Whether it mentions penalties or sanctions (true/false)\n",
    "6. \"references_other_sections\": Whether it references other legal sections (true/false)\n",
    "7. \"actionable_requirements\": Whether it contains specific requirements or obligations (true/false)\n",
    "8. \"confidence_score\": Your confidence in this classification (0.0 to 1.0)\n",
    "9. \"keywords\": Key legal terms found in the text (array)\n",
    "10. \"summary\": Brief summary of the content (1-2 sentences)\n",
    "\n",
    "Respond with valid JSON only.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def label_with_openai(ai_system, text: str, element_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"Label content using OpenAI GPT\"\"\"\n",
    "    try:\n",
    "        prompt = create_labeling_prompt(text, element_type)\n",
    "        \n",
    "        response = ai_system.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # Use GPT-4 for better accuracy\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal document analysis expert. Always respond with valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.1  # Low temperature for consistent results\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        result['model_used'] = 'gpt-4'\n",
    "        result['processing_time'] = response.usage.total_tokens if hasattr(response, 'usage') else None\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"OpenAI labeling failed: {e}\")\n",
    "        return {\n",
    "            \"content_category\": \"unknown\",\n",
    "            \"error\": str(e),\n",
    "            \"model_used\": \"gpt-4\"\n",
    "        }\n",
    "\n",
    "def label_with_gemini(ai_system, text: str, element_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"Label content using Google Gemini\"\"\"\n",
    "    try:\n",
    "        prompt = create_labeling_prompt(text, element_type)\n",
    "        \n",
    "        response = ai_system.gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=500,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        content = response.text.strip()\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        result['model_used'] = 'gemini-1.5-flash'\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gemini labeling failed: {e}\")\n",
    "        return {\n",
    "            \"content_category\": \"unknown\", \n",
    "            \"error\": str(e),\n",
    "            \"model_used\": \"gemini-1.5-flash\"\n",
    "        }\n",
    "\n",
    "def smart_label_content(ai_system, text: str, element_type: str, preferred_model: str = \"auto\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Intelligently label content using the best available model\n",
    "    \n",
    "    Args:\n",
    "        ai_system: AILabelingSystem instance\n",
    "        text: Text content to label\n",
    "        element_type: Type of element from unstructured\n",
    "        preferred_model: \"openai\", \"gemini\", or \"auto\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Skip very short text\n",
    "    if len(text.strip()) < 10:\n",
    "        return {\n",
    "            \"content_category\": \"minimal_content\",\n",
    "            \"confidence_score\": 0.9,\n",
    "            \"model_used\": \"rule_based\"\n",
    "        }\n",
    "    \n",
    "    # Choose model\n",
    "    if preferred_model == \"auto\":\n",
    "        # Prefer OpenAI for complex analysis, Gemini for speed\n",
    "        if ai_system.openai_client and len(text) > 200:\n",
    "            preferred_model = \"openai\"\n",
    "        elif ai_system.gemini_model:\n",
    "            preferred_model = \"gemini\"\n",
    "        elif ai_system.openai_client:\n",
    "            preferred_model = \"openai\"\n",
    "    \n",
    "    # Label with chosen model\n",
    "    if preferred_model == \"openai\" and ai_system.openai_client:\n",
    "        return label_with_openai(ai_system, text, element_type)\n",
    "    elif preferred_model == \"gemini\" and ai_system.gemini_model:\n",
    "        return label_with_gemini(ai_system, text, element_type)\n",
    "    else:\n",
    "        logger.warning(\"No AI model available for labeling\")\n",
    "        return {\n",
    "            \"content_category\": \"unlabeled\",\n",
    "            \"error\": \"No AI model available\",\n",
    "            \"model_used\": \"none\"\n",
    "        }\n",
    "\n",
    "print(\"âœ… Context labeling functions created!\")\n",
    "print(\"ðŸŽ¯ Available functions:\")\n",
    "print(\"   â€¢ smart_label_content() - Auto-select best model\")\n",
    "print(\"   â€¢ label_with_openai() - Use GPT-4\")\n",
    "print(\"   â€¢ label_with_gemini() - Use Gemini\")\n",
    "print(\"\\\\nðŸ§ª Test with a sample:\")\n",
    "\n",
    "# Test the labeling system\n",
    "sample_text = \"31A. Establishment of the Commission\\\\n(1) There is established a commission to be known as the Malaysian Communications and Multimedia Commission.\"\n",
    "sample_result = smart_label_content(ai_system, sample_text, \"NarrativeText\")\n",
    "print(f\"Sample result: {json.dumps(sample_result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d5ff0",
   "metadata": {},
   "source": [
    "## ðŸ”„ Process PDF Elements with AI\n",
    "\n",
    "Extract PDF content and apply intelligent AI labeling to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_ai_labeling(\n",
    "    pdf_path: str, \n",
    "    max_pages: Optional[int] = None,\n",
    "    preferred_model: str = \"auto\",\n",
    "    rate_limit_delay: float = 1.0,\n",
    "    batch_size: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Extract PDF content and label with AI\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        max_pages: Max pages to process\n",
    "        preferred_model: \"openai\", \"gemini\", or \"auto\"\n",
    "        rate_limit_delay: Delay between API calls (seconds)\n",
    "        batch_size: Process elements in batches\n",
    "    \n",
    "    Returns:\n",
    "        Complete analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"ðŸš€ Starting AI-powered PDF analysis: {Path(pdf_path).name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Extract PDF content\n",
    "    elements = extract_pdf_content(pdf_path, max_pages)\n",
    "    if not elements:\n",
    "        return {\"error\": \"Failed to extract PDF content\", \"elements\": []}\n",
    "    \n",
    "    logger.info(f\"ðŸ“„ Processing {len(elements)} elements with AI labeling...\")\n",
    "    \n",
    "    # Step 2: Process elements with AI\n",
    "    labeled_elements = []\n",
    "    api_calls_made = 0\n",
    "    errors = 0\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    for i in tqdm(range(0, len(elements), batch_size), desc=\"ðŸ¤– AI Labeling\"):\n",
    "        batch = elements[i:i + batch_size]\n",
    "        \n",
    "        for element in batch:\n",
    "            try:\n",
    "                # AI labeling\n",
    "                ai_label = smart_label_content(\n",
    "                    ai_system, \n",
    "                    element['text'], \n",
    "                    element['element_type'], \n",
    "                    preferred_model\n",
    "                )\n",
    "                \n",
    "                # Combine original element with AI labels\n",
    "                labeled_element = {\n",
    "                    **element,  # Original element data\n",
    "                    'ai_labels': ai_label,  # AI-generated labels\n",
    "                    'processing_timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                labeled_elements.append(labeled_element)\n",
    "                api_calls_made += 1\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(rate_limit_delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing element {element['index']}: {e}\")\n",
    "                errors += 1\n",
    "                \n",
    "                # Add element without AI labels\n",
    "                labeled_elements.append({\n",
    "                    **element,\n",
    "                    'ai_labels': {'error': str(e), 'content_category': 'processing_failed'},\n",
    "                    'processing_timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    # Step 3: Generate analysis summary\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    categories = {}\n",
    "    high_importance = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for element in labeled_elements:\n",
    "        ai_labels = element.get('ai_labels', {})\n",
    "        category = ai_labels.get('content_category', 'unknown')\n",
    "        categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        if ai_labels.get('legal_significance') == 'high':\n",
    "            high_importance += 1\n",
    "        \n",
    "        total_chars += element.get('character_count', 0)\n",
    "    \n",
    "    # Compile final results\n",
    "    results = {\n",
    "        'document_info': {\n",
    "            'source_file': Path(pdf_path).name,\n",
    "            'total_elements': len(labeled_elements),\n",
    "            'total_characters': total_chars,\n",
    "            'pages_processed': max_pages if max_pages else \"all\",\n",
    "            'processing_time_seconds': round(processing_time, 2),\n",
    "            'api_calls_made': api_calls_made,\n",
    "            'errors': errors\n",
    "        },\n",
    "        'analysis_summary': {\n",
    "            'content_categories': categories,\n",
    "            'high_importance_elements': high_importance,\n",
    "            'category_distribution': {k: round(v/len(labeled_elements)*100, 1) for k, v in categories.items()},\n",
    "            'avg_chars_per_element': round(total_chars / len(labeled_elements)) if labeled_elements else 0\n",
    "        },\n",
    "        'labeled_elements': labeled_elements\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"âœ… Analysis complete! Processed {len(labeled_elements)} elements in {processing_time:.1f}s\")\n",
    "    return results\n",
    "\n",
    "# Example processing function\n",
    "def run_example_analysis(pdf_path: str = \"/content/sample_legal_document.pdf\"):\n",
    "    \"\"\"Run a complete example analysis\"\"\"\n",
    "    \n",
    "    if not Path(pdf_path).exists():\n",
    "        print(f\"âŒ PDF not found: {pdf_path}\")\n",
    "        print(\"ðŸ“ Please upload your PDF to Colab and update the path\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ” Analyzing: {pdf_path}\")\n",
    "    print(\"âš™ï¸  Settings: First 3 pages, auto model selection, 1s rate limit\")\n",
    "    \n",
    "    # Run analysis\n",
    "    results = process_pdf_with_ai_labeling(\n",
    "        pdf_path=pdf_path,\n",
    "        max_pages=3,  # First 3 pages only\n",
    "        preferred_model=\"auto\",\n",
    "        rate_limit_delay=1.0,\n",
    "        batch_size=5\n",
    "    )\n",
    "    \n",
    "    if results and 'document_info' in results:\n",
    "        # Display summary\n",
    "        info = results['document_info']\n",
    "        summary = results['analysis_summary']\n",
    "        \n",
    "        print(f\"\\\\nðŸ“Š ANALYSIS RESULTS:\")\n",
    "        print(f\"ðŸ“„ Elements processed: {info['total_elements']}\")\n",
    "        print(f\"â±ï¸  Processing time: {info['processing_time_seconds']}s\")\n",
    "        print(f\"ðŸ¤– API calls made: {info['api_calls_made']}\")\n",
    "        print(f\"âš ï¸  Errors: {info['errors']}\")\n",
    "        \n",
    "        print(f\"\\\\nðŸ·ï¸  CONTENT CATEGORIES:\")\n",
    "        for category, count in summary['content_categories'].items():\n",
    "            percentage = summary['category_distribution'][category]\n",
    "            print(f\"   â€¢ {category}: {count} ({percentage}%)\")\n",
    "        \n",
    "        print(f\"\\\\nðŸŽ¯ High importance elements: {summary['high_importance_elements']}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"âœ… PDF processing functions ready!\")\n",
    "print(\"\\\\nðŸš€ To run analysis:\")\n",
    "print(\"1. Upload your PDF to Colab\")\n",
    "print(\"2. Update the pdf_path in run_example_analysis()\")\n",
    "print(\"3. Call: results = run_example_analysis()\")\n",
    "\n",
    "# Uncomment to run with your PDF\n",
    "# results = run_example_analysis(\"/content/your_legal_document.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875eee9",
   "metadata": {},
   "source": [
    "## âš¡ Batch Processing and Optimization\n",
    "\n",
    "Advanced techniques for processing multiple PDFs efficiently with caching and GPU optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class OptimizedPDFProcessor:\n",
    "    \"\"\"Advanced PDF processor with caching and batch optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"/content/pdf_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.processed_files = {}\n",
    "        \n",
    "    def get_file_hash(self, file_path: str) -> str:\n",
    "        \"\"\"Generate hash for file caching\"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return hashlib.md5(f.read()).hexdigest()\n",
    "    \n",
    "    def load_from_cache(self, file_hash: str) -> Optional[Dict]:\n",
    "        \"\"\"Load processed results from cache\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{file_hash}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Cache load failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def save_to_cache(self, file_hash: str, results: Dict):\n",
    "        \"\"\"Save results to cache\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{file_hash}.pkl\"\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cache save failed: {e}\")\n",
    "    \n",
    "    def process_single_pdf_optimized(\n",
    "        self, \n",
    "        pdf_path: str,\n",
    "        use_cache: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict:\n",
    "        \"\"\"Process single PDF with caching\"\"\"\n",
    "        \n",
    "        file_hash = self.get_file_hash(pdf_path)\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache:\n",
    "            cached_result = self.load_from_cache(file_hash)\n",
    "            if cached_result:\n",
    "                logger.info(f\"ðŸ“ Loaded from cache: {Path(pdf_path).name}\")\n",
    "                return cached_result\n",
    "        \n",
    "        # Process if not cached\n",
    "        logger.info(f\"ðŸ”„ Processing (not cached): {Path(pdf_path).name}\")\n",
    "        results = process_pdf_with_ai_labeling(pdf_path, **kwargs)\n",
    "        \n",
    "        # Save to cache\n",
    "        if use_cache and results:\n",
    "            self.save_to_cache(file_hash, results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_process_pdfs(\n",
    "        self,\n",
    "        pdf_paths: List[str],\n",
    "        max_workers: int = 3,  # Conservative for API rate limits\n",
    "        use_cache: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple PDFs in parallel with optimization\"\"\"\n",
    "        \n",
    "        logger.info(f\"ðŸš€ Starting batch processing of {len(pdf_paths)} PDFs\")\n",
    "        \n",
    "        results = {}\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        # Process with controlled parallelism\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all jobs\n",
    "            future_to_path = {\n",
    "                executor.submit(\n",
    "                    self.process_single_pdf_optimized, \n",
    "                    pdf_path, \n",
    "                    use_cache, \n",
    "                    **kwargs\n",
    "                ): pdf_path for pdf_path in pdf_paths\n",
    "            }\n",
    "            \n",
    "            # Collect results with progress bar\n",
    "            for future in tqdm(as_completed(future_to_path), total=len(pdf_paths), desc=\"ðŸ“„ Processing PDFs\"):\n",
    "                pdf_path = future_to_path[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[Path(pdf_path).name] = result\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {pdf_path}: {e}\")\n",
    "                    results[Path(pdf_path).name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Generate batch summary\n",
    "        total_time = time.time() - total_start_time\n",
    "        successful = sum(1 for r in results.values() if \"error\" not in r)\n",
    "        failed = len(results) - successful\n",
    "        \n",
    "        batch_summary = {\n",
    "            'batch_info': {\n",
    "                'total_files': len(pdf_paths),\n",
    "                'successful': successful,\n",
    "                'failed': failed,\n",
    "                'total_processing_time': round(total_time, 2),\n",
    "                'avg_time_per_file': round(total_time / len(pdf_paths), 2)\n",
    "            },\n",
    "            'file_results': results\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"âœ… Batch complete: {successful}/{len(pdf_paths)} successful in {total_time:.1f}s\")\n",
    "        return batch_summary\n",
    "\n",
    "def demonstrate_optimization():\n",
    "    \"\"\"Demonstrate advanced processing features\"\"\"\n",
    "    \n",
    "    print(\"âš¡ OPTIMIZATION FEATURES:\")\n",
    "    print(\"\\\\n1. ðŸ“ Intelligent Caching:\")\n",
    "    print(\"   â€¢ Files are hashed to detect changes\")\n",
    "    print(\"   â€¢ Processed results cached to disk\")\n",
    "    print(\"   â€¢ Automatic cache invalidation\")\n",
    "    \n",
    "    print(\"\\\\n2. ðŸ”„ Parallel Processing:\")\n",
    "    print(\"   â€¢ Multiple PDFs processed simultaneously\")\n",
    "    print(\"   â€¢ Controlled concurrency for API limits\")\n",
    "    print(\"   â€¢ Progress tracking and error handling\")\n",
    "    \n",
    "    print(\"\\\\n3. ðŸŽ¯ Memory Optimization:\")\n",
    "    print(\"   â€¢ Batch processing to manage memory\")\n",
    "    print(\"   â€¢ Garbage collection between files\")\n",
    "    print(\"   â€¢ GPU memory monitoring\")\n",
    "    \n",
    "    print(\"\\\\n4. ðŸ“Š Performance Monitoring:\")\n",
    "    print(\"   â€¢ Processing time tracking\")\n",
    "    print(\"   â€¢ API call counting\")\n",
    "    print(\"   â€¢ Success/failure rates\")\n",
    "    \n",
    "    # Example usage\n",
    "    processor = OptimizedPDFProcessor()\n",
    "    \n",
    "    print(f\"\\\\nðŸ’¾ Cache directory: {processor.cache_dir}\")\n",
    "    print(f\"ðŸ“ Cache size: {len(list(processor.cache_dir.glob('*.pkl')))} files\")\n",
    "    \n",
    "    return processor\n",
    "\n",
    "# Initialize optimized processor\n",
    "processor = demonstrate_optimization()\n",
    "\n",
    "# Example batch processing\n",
    "def run_batch_example(pdf_directory: str = \"/content/legal_pdfs/\"):\n",
    "    \"\"\"Example of batch processing multiple PDFs\"\"\"\n",
    "    \n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    if not pdf_dir.exists():\n",
    "        print(f\"âŒ Directory not found: {pdf_directory}\")\n",
    "        print(\"ðŸ“ Create directory and upload PDFs to test batch processing\")\n",
    "        return\n",
    "    \n",
    "    # Find all PDFs\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"âŒ No PDF files found in {pdf_directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ” Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    # Run batch processing\n",
    "    batch_results = processor.batch_process_pdfs(\n",
    "        pdf_paths=[str(p) for p in pdf_files],\n",
    "        max_workers=2,  # Conservative for API limits\n",
    "        max_pages=2,    # First 2 pages only for demo\n",
    "        preferred_model=\"auto\",\n",
    "        rate_limit_delay=0.5\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\\\nðŸ“Š BATCH RESULTS:\")\n",
    "    info = batch_results['batch_info']\n",
    "    print(f\"âœ… Successful: {info['successful']}/{info['total_files']}\")\n",
    "    print(f\"â±ï¸  Total time: {info['total_processing_time']}s\")\n",
    "    print(f\"ðŸ“ˆ Avg per file: {info['avg_time_per_file']}s\")\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "print(\"\\\\nðŸš€ Ready for optimized processing!\")\n",
    "print(\"ðŸ’¡ Tips for Google Colab:\")\n",
    "print(\"   â€¢ Use GPU runtime for faster processing\")\n",
    "print(\"   â€¢ Enable high-RAM if processing many files\")\n",
    "print(\"   â€¢ Monitor API usage to avoid rate limits\")\n",
    "print(\"   â€¢ Cache results to avoid reprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368b0cb",
   "metadata": {},
   "source": [
    "## ðŸ“Š Export Results and Visualization\n",
    "\n",
    "Save labeled PDF content and create insightful visualizations of the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e17ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_to_formats(results: Dict, output_dir: str = \"/content/parsed/EN\"):\n",
    "    \"\"\"Export Malaysian legal analysis results to multiple formats in parsed folder structure\"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if 'document_info' not in results:\n",
    "        logger.error(\"Invalid results format\")\n",
    "        return\n",
    "    \n",
    "    doc_info = results['document_info']\n",
    "    doc_name = doc_info['source_file'].replace('.pdf', '')\n",
    "    language = doc_info.get('language', 'unknown')\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Complete JSON export (this is the main output for parsed folder)\n",
    "    json_file = output_path / f\"{doc_name}_complete_analysis.json\"\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 2. Structured CSV for legal review and analysis\n",
    "    elements_data = []\n",
    "    for element in results['labeled_elements']:\n",
    "        ai_labels = element.get('ai_labels', {})\n",
    "        row = {\n",
    "            'document_name': doc_name,\n",
    "            'language': language,\n",
    "            'index': element['index'],\n",
    "            'page_number': element['page_number'],\n",
    "            'element_type': element['element_type'],\n",
    "            'character_count': element['character_count'],\n",
    "            'content_category': ai_labels.get('content_category', 'unknown'),\n",
    "            'legal_significance': ai_labels.get('legal_significance', 'unknown'),\n",
    "            'subject_matter': ai_labels.get('subject_matter', 'unknown'),\n",
    "            'confidence_score': ai_labels.get('confidence_score', 0),\n",
    "            'contains_definitions': ai_labels.get('contains_definitions', False),\n",
    "            'contains_penalties': ai_labels.get('contains_penalties', False),\n",
    "            'actionable_requirements': ai_labels.get('actionable_requirements', False),\n",
    "            'references_other_sections': ai_labels.get('references_other_sections', False),\n",
    "            'model_used': ai_labels.get('model_used', 'unknown'),\n",
    "            'keywords': ', '.join(ai_labels.get('keywords', [])) if ai_labels.get('keywords') else '',\n",
    "            'summary': ai_labels.get('summary', ''),\n",
    "            'text_preview': element['text'][:200] + \"...\" if len(element['text']) > 200 else element['text']\n",
    "        }\n",
    "        elements_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(elements_data)\n",
    "    csv_file = output_path / f\"{doc_name}_legal_analysis.csv\"\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # 3. Legal summary report for human review\n",
    "    report_file = output_path / f\"{doc_name}_legal_summary.txt\"\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        info = results['document_info']\n",
    "        summary = results['analysis_summary']\n",
    "        \n",
    "        f.write(f\"MALAYSIAN LEGAL DOCUMENT ANALYSIS REPORT\\\\n\")\n",
    "        f.write(f\"{'='*60}\\\\n\\\\n\")\n",
    "        f.write(f\"Document: {info['source_file']}\\\\n\")\n",
    "        f.write(f\"Language: {info.get('language', 'Unknown')}\\\\n\")\n",
    "        f.write(f\"Source Path: {info.get('source_path', 'Unknown')}\\\\n\")\n",
    "        f.write(f\"Processed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "        f.write(f\"Processing Time: {info['processing_time_seconds']}s\\\\n\")\n",
    "        f.write(f\"Extraction Method: {info.get('extraction_method', 'Unknown')}\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(f\"DOCUMENT STATISTICS:\\\\n\")\n",
    "        f.write(f\"{'-'*30}\\\\n\")\n",
    "        f.write(f\"Total Elements: {info['total_elements']}\\\\n\")\n",
    "        f.write(f\"Total Characters: {info['total_characters']:,}\\\\n\")\n",
    "        f.write(f\"API Calls Made: {info['api_calls_made']}\\\\n\")\n",
    "        f.write(f\"Models Used: {', '.join(info.get('models_used', []))}\\\\n\")\n",
    "        f.write(f\"Average Confidence: {summary['average_confidence']}\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(f\"LEGAL CONTENT ANALYSIS:\\\\n\")\n",
    "        f.write(f\"{'-'*30}\\\\n\")\n",
    "        for category, count in summary['content_categories'].items():\n",
    "            percentage = summary['category_distribution'][category]\n",
    "            f.write(f\"{category}: {count} elements ({percentage}%)\\\\n\")\n",
    "        \n",
    "        f.write(f\"\\\\nLEGAL SIGNIFICANCE:\\\\n\")\n",
    "        f.write(f\"{'-'*30}\\\\n\")\n",
    "        f.write(f\"High Importance Elements: {summary['high_importance_elements']}\\\\n\")\n",
    "        \n",
    "        # Extract high importance elements\n",
    "        high_importance_elements = [\n",
    "            el for el in results['labeled_elements'] \n",
    "            if el.get('ai_labels', {}).get('legal_significance') == 'high'\n",
    "        ]\n",
    "        \n",
    "        if high_importance_elements:\n",
    "            f.write(f\"\\\\nHIGH IMPORTANCE SECTIONS:\\\\n\")\n",
    "            f.write(f\"{'-'*30}\\\\n\")\n",
    "            for i, element in enumerate(high_importance_elements[:10]):  # Top 10\n",
    "                ai_labels = element.get('ai_labels', {})\n",
    "                f.write(f\"{i+1}. Page {element['page_number']} - {ai_labels.get('content_category', 'Unknown')}\\\\n\")\n",
    "                f.write(f\"   Subject: {ai_labels.get('subject_matter', 'Unknown')}\\\\n\")\n",
    "                f.write(f\"   Text: {element['text'][:150]}{'...' if len(element['text']) > 150 else ''}\\\\n\\\\n\")\n",
    "    \n",
    "    # 4. Create a metadata index file for the parsed folder\n",
    "    metadata_file = output_path / f\"{doc_name}_metadata.json\"\n",
    "    metadata = {\n",
    "        'document_info': doc_info,\n",
    "        'analysis_summary': summary,\n",
    "        'files_generated': {\n",
    "            'complete_analysis': json_file.name,\n",
    "            'csv_analysis': csv_file.name,\n",
    "            'text_summary': report_file.name,\n",
    "            'metadata': metadata_file.name\n",
    "        },\n",
    "        'generation_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logger.info(f\"âœ… Malaysian legal analysis exported to: {output_path}\")\n",
    "    logger.info(f\"ðŸ“„ Files created:\")\n",
    "    logger.info(f\"   â€¢ {json_file.name} (Complete analysis - main output)\")\n",
    "    logger.info(f\"   â€¢ {csv_file.name} (Structured data for review)\")\n",
    "    logger.info(f\"   â€¢ {report_file.name} (Human-readable summary)\")\n",
    "    logger.info(f\"   â€¢ {metadata_file.name} (Document metadata)\")\n",
    "    \n",
    "    return {\n",
    "        'json_file': str(json_file),\n",
    "        'csv_file': str(csv_file),\n",
    "        'report_file': str(report_file),\n",
    "        'metadata_file': str(metadata_file),\n",
    "        'dataframe': df,\n",
    "        'output_directory': str(output_path)\n",
    "    }\n",
    "\n",
    "def create_analysis_visualizations(results: Dict, output_dir: str = \"/content/parsed/EN\"):\n",
    "    \"\"\"Create comprehensive visualizations for Malaysian legal document analysis\"\"\"\n",
    "    \n",
    "    if 'labeled_elements' not in results:\n",
    "        logger.error(\"No labeled elements found for visualization\")\n",
    "        return\n",
    "    \n",
    "    doc_info = results['document_info']\n",
    "    doc_name = doc_info['source_file'].replace('.pdf', '')\n",
    "    language = doc_info.get('language', 'unknown')\n",
    "    \n",
    "    # Prepare data\n",
    "    elements_data = []\n",
    "    for element in results['labeled_elements']:\n",
    "        ai_labels = element.get('ai_labels', {})\n",
    "        elements_data.append({\n",
    "            'page': element['page_number'],\n",
    "            'element_type': element['element_type'],\n",
    "            'category': ai_labels.get('content_category', 'unknown'),\n",
    "            'significance': ai_labels.get('legal_significance', 'unknown'),\n",
    "            'confidence': ai_labels.get('confidence_score', 0),\n",
    "            'char_count': element['character_count'],\n",
    "            'has_definitions': ai_labels.get('contains_definitions', False),\n",
    "            'has_penalties': ai_labels.get('contains_penalties', False),\n",
    "            'has_requirements': ai_labels.get('actionable_requirements', False)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(elements_data)\n",
    "    \n",
    "    # Create enhanced visualizations for legal documents\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle(f'Malaysian Legal Document Analysis: {doc_name} ({language})', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Content Categories Distribution\n",
    "    category_counts = df['category'].value_counts()\n",
    "    colors = plt.cm.Set3(range(len(category_counts)))\n",
    "    axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', \n",
    "                  startangle=90, colors=colors)\n",
    "    axes[0, 0].set_title('Legal Content Categories')\n",
    "    \n",
    "    # 2. Legal Significance Distribution\n",
    "    significance_counts = df['significance'].value_counts()\n",
    "    colors_sig = {'high': 'red', 'medium': 'orange', 'low': 'green', 'unknown': 'gray'}\n",
    "    bar_colors = [colors_sig.get(sig, 'gray') for sig in significance_counts.index]\n",
    "    bars = axes[0, 1].bar(significance_counts.index, significance_counts.values, color=bar_colors)\n",
    "    axes[0, 1].set_title('Legal Significance Levels')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # 3. Elements Distribution by Page\n",
    "    page_counts = df['page'].value_counts().sort_index()\n",
    "    axes[0, 2].plot(page_counts.index, page_counts.values, marker='o', linewidth=2, markersize=6, color='blue')\n",
    "    axes[0, 2].set_title('Elements per Page')\n",
    "    axes[0, 2].set_xlabel('Page Number')\n",
    "    axes[0, 2].set_ylabel('Element Count')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Element Types Distribution\n",
    "    type_counts = df['element_type'].value_counts()\n",
    "    axes[1, 0].barh(type_counts.index, type_counts.values, color='skyblue')\n",
    "    axes[1, 0].set_title('Document Element Types')\n",
    "    axes[1, 0].set_xlabel('Count')\n",
    "    \n",
    "    # 5. AI Confidence Score Distribution\n",
    "    axes[1, 1].hist(df['confidence'], bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_title('AI Confidence Score Distribution')\n",
    "    axes[1, 1].set_xlabel('Confidence Score')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    mean_conf = df['confidence'].mean()\n",
    "    axes[1, 1].axvline(mean_conf, color='red', linestyle='--', label=f'Mean: {mean_conf:.2f}')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # 6. Character Count vs Confidence (colored by page)\n",
    "    scatter = axes[1, 2].scatter(df['char_count'], df['confidence'], c=df['page'], \n",
    "                               cmap='viridis', alpha=0.6, s=30)\n",
    "    axes[1, 2].set_title('Content Length vs AI Confidence')\n",
    "    axes[1, 2].set_xlabel('Character Count')\n",
    "    axes[1, 2].set_ylabel('Confidence Score')\n",
    "    plt.colorbar(scatter, ax=axes[1, 2], label='Page Number')\n",
    "    \n",
    "    # 7. Legal Features Analysis\n",
    "    features = ['has_definitions', 'has_penalties', 'has_requirements']\n",
    "    feature_counts = [df[feature].sum() for feature in features]\n",
    "    feature_labels = ['Contains Definitions', 'Contains Penalties', 'Has Requirements']\n",
    "    axes[2, 0].bar(feature_labels, feature_counts, color=['purple', 'orange', 'green'])\n",
    "    axes[2, 0].set_title('Legal Features Detection')\n",
    "    axes[2, 0].set_ylabel('Count')\n",
    "    plt.setp(axes[2, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 8. High Importance Elements by Page\n",
    "    high_importance = df[df['significance'] == 'high']\n",
    "    if not high_importance.empty:\n",
    "        high_by_page = high_importance['page'].value_counts().sort_index()\n",
    "        axes[2, 1].bar(high_by_page.index, high_by_page.values, color='red', alpha=0.7)\n",
    "        axes[2, 1].set_title('High Importance Elements by Page')\n",
    "        axes[2, 1].set_xlabel('Page Number')\n",
    "        axes[2, 1].set_ylabel('High Importance Count')\n",
    "    else:\n",
    "        axes[2, 1].text(0.5, 0.5, 'No High Importance\\\\nElements Found', \n",
    "                       ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "        axes[2, 1].set_title('High Importance Elements by Page')\n",
    "    \n",
    "    # 9. Category vs Significance Heatmap\n",
    "    if len(df) > 0:\n",
    "        pivot_table = df.pivot_table(values='confidence', index='category', \n",
    "                                   columns='significance', aggfunc='count', fill_value=0)\n",
    "        if not pivot_table.empty:\n",
    "            im = axes[2, 2].imshow(pivot_table.values, cmap='YlOrRd', aspect='auto')\n",
    "            axes[2, 2].set_xticks(range(len(pivot_table.columns)))\n",
    "            axes[2, 2].set_yticks(range(len(pivot_table.index)))\n",
    "            axes[2, 2].set_xticklabels(pivot_table.columns)\n",
    "            axes[2, 2].set_yticklabels(pivot_table.index)\n",
    "            axes[2, 2].set_title('Category vs Significance Heatmap')\n",
    "            plt.colorbar(im, ax=axes[2, 2])\n",
    "        else:\n",
    "            axes[2, 2].text(0.5, 0.5, 'Insufficient Data\\\\nfor Heatmap', \n",
    "                           ha='center', va='center', transform=axes[2, 2].transAxes)\n",
    "            axes[2, 2].set_title('Category vs Significance Heatmap')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    output_path = Path(output_dir)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    viz_file = output_path / f\"{doc_name}_legal_analysis_dashboard.png\"\n",
    "    plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"ðŸ“Š Legal analysis dashboard saved: {viz_file}\")\n",
    "    \n",
    "    # Enhanced analysis summary\n",
    "    print(f\"\\\\n\udcca MALAYSIAN LEGAL DOCUMENT ANALYSIS SUMMARY:\")\n",
    "    print(f\"ðŸ“„ Document: {doc_name} ({language})\")\n",
    "    print(f\"ðŸ“ˆ Total elements: {len(df)}\")\n",
    "    print(f\"ðŸŽ¯ Average AI confidence: {df['confidence'].mean():.2f}\")\n",
    "    print(f\"ðŸ“ Average chars per element: {df['char_count'].mean():.0f}\")\n",
    "    print(f\"â­ High confidence elements (>0.8): {len(df[df['confidence'] > 0.8])}\")\n",
    "    print(f\"ðŸš¨ High importance elements: {len(df[df['significance'] == 'high'])}\")\n",
    "    \n",
    "    print(f\"\\\\nðŸ·ï¸  TOP LEGAL CATEGORIES:\")\n",
    "    for cat, count in category_counts.head().items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   â€¢ {cat}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\nâš–ï¸  LEGAL FEATURES DETECTED:\")\n",
    "    print(f\"   â€¢ Documents with definitions: {df['has_definitions'].sum()}\")\n",
    "    print(f\"   â€¢ Documents with penalties: {df['has_penalties'].sum()}\")\n",
    "    print(f\"   â€¢ Documents with requirements: {df['has_requirements'].sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def demonstrate_malaysian_legal_export():\n",
    "    \"\"\"Demonstrate export workflow for Malaysian legal documents\"\"\"\n",
    "    \n",
    "    print(\"ðŸ“Š MALAYSIAN LEGAL DOCUMENT EXPORT WORKFLOW:\")\n",
    "    print(\"\\\\n1. ðŸ“„ Structured Output for Parsed Folder:\")\n",
    "    print(\"   â€¢ Complete JSON analysis (main output)\")\n",
    "    print(\"   â€¢ CSV file for legal review and analysis\")\n",
    "    print(\"   â€¢ Human-readable summary report\")\n",
    "    print(\"   â€¢ Metadata index for document tracking\")\n",
    "    \n",
    "    print(\"\\\\n2. ðŸ“ˆ Legal-Specific Visualizations:\")\n",
    "    print(\"   â€¢ Content category analysis\")\n",
    "    print(\"   â€¢ Legal significance assessment\")\n",
    "    print(\"   â€¢ Feature detection (definitions, penalties, requirements)\")\n",
    "    print(\"   â€¢ Page-by-page importance mapping\")\n",
    "    print(\"   â€¢ AI confidence and quality metrics\")\n",
    "    \n",
    "    print(\"\\\\n3. ðŸŽ¯ Legal Analysis Features:\")\n",
    "    print(\"   â€¢ High importance section identification\")\n",
    "    print(\"   â€¢ Legal terminology detection\")\n",
    "    print(\"   â€¢ Cross-reference analysis\")\n",
    "    print(\"   â€¢ Language-aware processing\")\n",
    "    \n",
    "    print(\"\\\\nðŸ’¡ Usage for Malaysian Legal Acts:\")\n",
    "    print(\"   # Process single document\")\n",
    "    print(\"   results = process_pdf_with_ai_labeling('malaysian_acts/EN/act125.pdf')\")\n",
    "    print(\"   export_data = export_results_to_formats(results, 'parsed/EN')\")\n",
    "    print(\"   df = create_analysis_visualizations(results, 'parsed/EN')\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "demonstrate_malaysian_legal_export()\n",
    "\n",
    "print(\"\\\\nâœ… Malaysian legal document export functions ready!\")\n",
    "print(\"\udfdbï¸  Optimized for legal document analysis and parsed folder structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92442af4",
   "metadata": {},
   "source": [
    "## ðŸš€ Complete Processing Workflow\n",
    "\n",
    "Now we'll put everything together into a complete workflow that processes your Malaysian legal PDFs with AI-powered content labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def complete_malaysian_legal_analysis(\n",
    "    pdf_path: str, \n",
    "    use_openai: bool = True,\n",
    "    use_gemini: bool = False,\n",
    "    output_dir: str = \"/content/parsed\",\n",
    "    language: str = \"auto\"  # \"EN\", \"BM\", or \"auto\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete workflow for analyzing Malaysian legal PDFs with AI labeling\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file (from malaysian_acts folder)\n",
    "        use_openai: Whether to use OpenAI GPT-4 for labeling\n",
    "        use_gemini: Whether to use Google Gemini for labeling\n",
    "        output_dir: Directory to save results (parsed folder)\n",
    "        language: Document language (\"EN\", \"BM\", or \"auto\" for detection)\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing complete analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"ðŸ›ï¸  Starting Malaysian Legal PDF Analysis: {Path(pdf_path).name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Detect language if auto\n",
    "        if language == \"auto\":\n",
    "            if '/EN/' in pdf_path.upper() or '/en/' in pdf_path.lower():\n",
    "                language = \"EN\"\n",
    "            elif '/BM/' in pdf_path.upper() or '/bm/' in pdf_path.lower():\n",
    "                language = \"BM\"\n",
    "            else:\n",
    "                language = \"other\"\n",
    "        \n",
    "        print(f\"ðŸŒ Document language: {language}\")\n",
    "        \n",
    "        # Step 2: Extract PDF content with unstructured\n",
    "        print(\"ðŸ“„ Step 1: Extracting PDF content...\")\n",
    "        elements = extract_pdf_content(pdf_path)\n",
    "        if not elements:\n",
    "            raise ValueError(\"No elements extracted from PDF\")\n",
    "        \n",
    "        print(f\"âœ… Extracted {len(elements)} elements\")\n",
    "        \n",
    "        # Step 3: Set up AI models\n",
    "        print(\"ðŸ¤– Step 2: Setting up AI models...\")\n",
    "        \n",
    "        clients = {}\n",
    "        if use_openai and ai_system.openai_client:\n",
    "            clients['openai'] = ai_system.openai_client\n",
    "            print(\"âœ… OpenAI GPT-4 ready\")\n",
    "        \n",
    "        if use_gemini and ai_system.gemini_model:\n",
    "            clients['gemini'] = ai_system.gemini_model\n",
    "            print(\"âœ… Google Gemini ready\")\n",
    "        \n",
    "        if not clients:\n",
    "            raise ValueError(\"No AI models available. Please configure API keys.\")\n",
    "        \n",
    "        # Step 4: Process elements with AI labeling\n",
    "        print(\"ðŸ·ï¸  Step 3: AI content labeling...\")\n",
    "        \n",
    "        labeled_elements = []\n",
    "        total_api_calls = 0\n",
    "        \n",
    "        for i, element in enumerate(elements):\n",
    "            print(f\"Processing element {i+1}/{len(elements)} (Page {element['page_number']})...\", end=\"\\\\r\")\n",
    "            \n",
    "            # Choose AI model (prefer OpenAI for legal documents)\n",
    "            if 'openai' in clients:\n",
    "                ai_labels = smart_label_content(ai_system, element['text'], element['element_type'], \"openai\")\n",
    "                if ai_labels and 'error' not in ai_labels:\n",
    "                    ai_labels['model_used'] = 'gpt-4'\n",
    "                    total_api_calls += 1\n",
    "            elif 'gemini' in clients:\n",
    "                ai_labels = smart_label_content(ai_system, element['text'], element['element_type'], \"gemini\")\n",
    "                if ai_labels and 'error' not in ai_labels:\n",
    "                    ai_labels['model_used'] = 'gemini-pro'\n",
    "                    total_api_calls += 1\n",
    "            else:\n",
    "                ai_labels = {}\n",
    "            \n",
    "            element['ai_labels'] = ai_labels\n",
    "            labeled_elements.append(element)\n",
    "            \n",
    "            # Rate limiting\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                time.sleep(1)  # Brief pause every 10 elements\n",
    "        \n",
    "        print(f\"\\\\nâœ… Completed AI labeling with {total_api_calls} API calls\")\n",
    "        \n",
    "        # Step 5: Create analysis summary\n",
    "        print(\"ðŸ“Š Step 4: Generating analysis summary...\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_chars = sum(len(el['text']) for el in labeled_elements)\n",
    "        content_categories = {}\n",
    "        high_importance = 0\n",
    "        category_distribution = {}\n",
    "        \n",
    "        for element in labeled_elements:\n",
    "            ai_labels = element.get('ai_labels', {})\n",
    "            category = ai_labels.get('content_category', 'unknown')\n",
    "            significance = ai_labels.get('legal_significance', 'unknown')\n",
    "            \n",
    "            content_categories[category] = content_categories.get(category, 0) + 1\n",
    "            \n",
    "            if significance == 'high':\n",
    "                high_importance += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_elements = len(labeled_elements)\n",
    "        for category, count in content_categories.items():\n",
    "            category_distribution[category] = round((count / total_elements) * 100, 1)\n",
    "        \n",
    "        # Step 6: Compile complete results\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        results = {\n",
    "            'document_info': {\n",
    "                'source_file': Path(pdf_path).name,\n",
    "                'source_path': str(pdf_path),\n",
    "                'language': language,\n",
    "                'total_elements': total_elements,\n",
    "                'total_characters': total_chars,\n",
    "                'processing_time_seconds': round(processing_time, 2),\n",
    "                'api_calls_made': total_api_calls,\n",
    "                'models_used': list(clients.keys()),\n",
    "                'processed_at': datetime.now().isoformat(),\n",
    "                'extraction_method': 'unstructured_hi_res'\n",
    "            },\n",
    "            'labeled_elements': labeled_elements,\n",
    "            'analysis_summary': {\n",
    "                'content_categories': content_categories,\n",
    "                'category_distribution': category_distribution,\n",
    "                'high_importance_elements': high_importance,\n",
    "                'average_confidence': round(\n",
    "                    sum(el.get('ai_labels', {}).get('confidence_score', 0) for el in labeled_elements) / total_elements, 3\n",
    "                ) if total_elements > 0 else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Step 7: Save to parsed folder with proper structure\n",
    "        print(\"ðŸ’¾ Step 5: Saving to parsed folder...\")\n",
    "        \n",
    "        # Create output directory structure\n",
    "        parsed_path = Path(output_dir)\n",
    "        lang_output_dir = parsed_path / language\n",
    "        lang_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save complete JSON analysis\n",
    "        output_filename = f\"{Path(pdf_path).stem}_analysis.json\"\n",
    "        output_file = lang_output_dir / output_filename\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"âœ… Analysis saved: {output_file}\")\n",
    "        \n",
    "        # Step 8: Export additional formats\n",
    "        print(\"ðŸ“Š Step 6: Creating additional exports...\")\n",
    "        export_data = export_results_to_formats(results, str(lang_output_dir))\n",
    "        \n",
    "        # Step 9: Create visualizations\n",
    "        print(\"ðŸ“ˆ Step 7: Creating visualizations...\")\n",
    "        df = create_analysis_visualizations(results, str(lang_output_dir))\n",
    "        \n",
    "        logger.info(f\"ðŸŽ‰ Analysis completed in {processing_time:.2f} seconds\")\n",
    "        logger.info(f\"ðŸ“Š Summary: {total_elements} elements, {high_importance} high importance\")\n",
    "        logger.info(f\"ðŸ“ Saved to: {output_file}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Analysis failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Updated demo function for Malaysian legal documents\n",
    "def quick_malaysian_legal_demo():\n",
    "    \"\"\"Demonstrate the analysis workflow specifically for Malaysian legal documents\"\"\"\n",
    "    \n",
    "    print(\"\udfdbï¸  MALAYSIAN LEGAL PDF AI ANALYSIS SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\\\nðŸ“‹ Specialized Features for Malaysian Legal Documents:\")\n",
    "    print(\"â€¢ ðŸ“„ PDF extraction optimized for legal formatting\")\n",
    "    print(\"â€¢ ðŸ¤– AI models trained to understand legal terminology\")\n",
    "    print(\"â€¢ ðŸ›ï¸  Malaysian legal document structure recognition\")\n",
    "    print(\"â€¢ ðŸŒ Language-aware processing (English/Bahasa Malaysia)\")\n",
    "    print(\"â€¢ ðŸ“Š Legal significance classification\")\n",
    "    print(\"â€¢ ðŸ’¾ Structured output in JSON format for parsed folder\")\n",
    "    print(\"â€¢ âš¡ Batch processing for multiple acts\")\n",
    "    \n",
    "    print(\"\\\\nðŸ“‚ Folder Structure:\")\n",
    "    print(\"   ðŸ“‚ malaysian_acts/ (Input)\")\n",
    "    print(\"      ðŸ“‚ EN/ (English legal documents)\")\n",
    "    print(\"      ðŸ“‚ BM/ (Bahasa Malaysia legal documents)\")\n",
    "    print(\"   ðŸ“‚ parsed/ (Output)\")\n",
    "    print(\"      ðŸ“‚ EN/ (English analysis results)\")\n",
    "    print(\"      ðŸ“‚ BM/ (Bahasa Malaysia analysis results)\")\n",
    "    \n",
    "    print(\"\\\\nðŸš€ Quick Start for Malaysian Legal Acts:\")\n",
    "    print(\"1. Ensure your malaysian_acts folder is uploaded to Colab\")\n",
    "    print(\"2. Configure API keys (OpenAI or Gemini)\")\n",
    "    print(\"3. Run batch processing or individual file analysis\")\n",
    "    print(\"4. Review results in the parsed folder\")\n",
    "    \n",
    "    print(\"\\\\nðŸ’¡ API Key Setup (Colab Secrets):\")\n",
    "    print(\"â€¢ In Colab: Go to ðŸ”‘ Secrets panel\")\n",
    "    print(\"â€¢ Add OPENAI_API_KEY and/or GEMINI_API_KEY\")\n",
    "    print(\"â€¢ Enable 'Notebook access' for each key\")\n",
    "    \n",
    "    print(\"\\\\nâš ï¸  Processing Notes:\")\n",
    "    print(\"â€¢ Legal documents are processed with high-resolution extraction\")\n",
    "    print(\"â€¢ AI models provide specialized legal classification\")\n",
    "    print(\"â€¢ Results include legal significance ratings\")\n",
    "    print(\"â€¢ Processing time varies with document complexity\")\n",
    "    print(\"â€¢ API costs apply for AI analysis\")\n",
    "    \n",
    "    print(\"\\\\nðŸŽ¯ Legal Classification Categories:\")\n",
    "    print(\"â€¢ section_header, legal_definition, procedural_requirement\")\n",
    "    print(\"â€¢ penalty_clause, schedule, interpretation, preamble\")\n",
    "    print(\"â€¢ Legal significance: high, medium, low\")\n",
    "    print(\"â€¢ Subject matter: criminal_procedure, corporate_law, etc.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "quick_malaysian_legal_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c4337",
   "metadata": {},
   "source": [
    "## ðŸ§ª Testing Your Malaysian Legal PDFs\n",
    "\n",
    "Ready to test the system! Upload your Malaysian legal PDF files and run the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ STEP 1: Discover Malaysian Legal PDFs\n",
    "# Scan the malaysian_acts folder for PDF files organized by language\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def discover_malaysian_acts(base_dir: str = \"/content/malaysian_acts\"):\n",
    "    \"\"\"Discover and categorize Malaysian legal PDFs by language\"\"\"\n",
    "    \n",
    "    malaysian_acts_path = Path(base_dir)\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not malaysian_acts_path.exists():\n",
    "        print(f\"âŒ Folder not found: {base_dir}\")\n",
    "        print(\"ðŸ“ Please upload your malaysian_acts folder to Colab\")\n",
    "        print(\"   Expected structure:\")\n",
    "        print(\"   \udcc2 malaysian_acts/\")\n",
    "        print(\"      ðŸ“‚ EN/ (English documents)\")\n",
    "        print(\"      ðŸ“‚ BM/ (Bahasa Malaysia documents)\")\n",
    "        return {}\n",
    "    \n",
    "    discovered_files = {\n",
    "        'EN': [],\n",
    "        'BM': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    # Scan for PDF files\n",
    "    for root, dirs, files in os.walk(malaysian_acts_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                file_path = Path(root) / file\n",
    "                \n",
    "                # Categorize by language folder\n",
    "                if 'EN' in str(file_path).upper() or '/en/' in str(file_path).lower():\n",
    "                    discovered_files['EN'].append(str(file_path))\n",
    "                elif 'BM' in str(file_path).upper() or '/bm/' in str(file_path).lower():\n",
    "                    discovered_files['BM'].append(str(file_path))\n",
    "                else:\n",
    "                    discovered_files['other'].append(str(file_path))\n",
    "    \n",
    "    # Display findings\n",
    "    total_files = sum(len(files) for files in discovered_files.values())\n",
    "    print(f\"ðŸ” DISCOVERED MALAYSIAN LEGAL PDFS:\")\n",
    "    print(f\"ðŸ“‚ Base directory: {base_dir}\")\n",
    "    print(f\"ðŸ“„ Total PDF files: {total_files}\")\n",
    "    print(f\"   â€¢ English (EN): {len(discovered_files['EN'])} files\")\n",
    "    print(f\"   â€¢ Bahasa Malaysia (BM): {len(discovered_files['BM'])} files\")\n",
    "    print(f\"   â€¢ Other/Uncategorized: {len(discovered_files['other'])} files\")\n",
    "    \n",
    "    # Show sample files\n",
    "    if discovered_files['EN']:\n",
    "        print(f\"\\\\n\udcc4 Sample EN files:\")\n",
    "        for file in discovered_files['EN'][:3]:\n",
    "            print(f\"   â€¢ {Path(file).name}\")\n",
    "    \n",
    "    if discovered_files['BM']:\n",
    "        print(f\"\\\\nðŸ“„ Sample BM files:\")\n",
    "        for file in discovered_files['BM'][:3]:\n",
    "            print(f\"   â€¢ {Path(file).name}\")\n",
    "    \n",
    "    return discovered_files\n",
    "\n",
    "def setup_parsed_output_directory(base_dir: str = \"/content/parsed\"):\n",
    "    \"\"\"Setup the parsed output directory with language categorization\"\"\"\n",
    "    \n",
    "    parsed_path = Path(base_dir)\n",
    "    parsed_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create language subdirectories\n",
    "    (parsed_path / \"EN\").mkdir(exist_ok=True)\n",
    "    (parsed_path / \"BM\").mkdir(exist_ok=True)\n",
    "    (parsed_path / \"other\").mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"ðŸ“ PARSED OUTPUT DIRECTORY SETUP:\")\n",
    "    print(f\"ðŸ“‚ Base directory: {base_dir}\")\n",
    "    print(f\"   ðŸ“‚ EN/ (English analysis results)\")\n",
    "    print(f\"   ðŸ“‚ BM/ (Bahasa Malaysia analysis results)\")\n",
    "    print(f\"   ðŸ“‚ other/ (Other language results)\")\n",
    "    \n",
    "    return parsed_path\n",
    "\n",
    "# ðŸŽ¯ STEP 2: Configure analysis for Malaysian legal documents\n",
    "def configure_malaysian_legal_analysis():\n",
    "    \"\"\"Configure analysis specifically for Malaysian legal documents\"\"\"\n",
    "    \n",
    "    print(\"ðŸ›ï¸  MALAYSIAN LEGAL DOCUMENT ANALYSIS CONFIGURATION:\")\n",
    "    print(\"\\\\nDocument Types Expected:\")\n",
    "    print(\"â€¢ Acts of Parliament\")\n",
    "    print(\"â€¢ Regulations and Rules\")\n",
    "    print(\"â€¢ Legal Guidelines\")\n",
    "    print(\"â€¢ Constitutional Documents\")\n",
    "    \n",
    "    # Check API key availability\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        openai_available = bool(userdata.get('OPENAI_API_KEY'))\n",
    "        gemini_available = bool(userdata.get('GEMINI_API_KEY'))\n",
    "    except:\n",
    "        openai_available = bool(os.getenv('OPENAI_API_KEY'))\n",
    "        gemini_available = bool(os.getenv('GEMINI_API_KEY'))\n",
    "    \n",
    "    print(f\"\\\\nðŸ”‘ API Key Status:\")\n",
    "    print(f\"   OpenAI: {'âœ… Available' if openai_available else 'âŒ Not configured'}\")\n",
    "    print(f\"   Gemini: {'âœ… Available' if gemini_available else 'âŒ Not configured'}\")\n",
    "    \n",
    "    if not openai_available and not gemini_available:\n",
    "        print(\"\\\\nâš ï¸  No API keys configured!\")\n",
    "        print(\"Please add your API keys to Colab Secrets:\")\n",
    "        print(\"1. Click ðŸ”‘ in the left sidebar\")\n",
    "        print(\"2. Add OPENAI_API_KEY and/or GEMINI_API_KEY\")\n",
    "        print(\"3. Enable 'Notebook access'\")\n",
    "        return None, None\n",
    "    \n",
    "    # Recommend OpenAI for legal document analysis\n",
    "    use_openai = openai_available\n",
    "    use_gemini = not openai_available and gemini_available\n",
    "    \n",
    "    print(f\"\\\\nðŸŽ¯ Recommended Configuration:\")\n",
    "    print(f\"   â€¢ OpenAI GPT-4: {'âœ… Enabled' if use_openai else 'âŒ Disabled'}\")\n",
    "    print(f\"   â€¢ Google Gemini: {'âœ… Enabled' if use_gemini else 'âŒ Disabled'}\")\n",
    "    print(f\"   â€¢ Optimized for Malaysian legal terminology\")\n",
    "    print(f\"   â€¢ Language-aware processing (EN/BM)\")\n",
    "    \n",
    "    return use_openai, use_gemini\n",
    "\n",
    "# ðŸš€ STEP 3: Batch process Malaysian legal documents\n",
    "async def process_malaysian_acts_folder(\n",
    "    acts_dir: str = \"/content/malaysian_acts\",\n",
    "    output_dir: str = \"/content/parsed\",\n",
    "    language_filter: str = \"all\",  # \"EN\", \"BM\", or \"all\"\n",
    "    max_files: int = None,\n",
    "    max_pages_per_file: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Process all Malaysian legal PDFs with language-aware analysis\n",
    "    \n",
    "    Args:\n",
    "        acts_dir: Directory containing malaysian_acts\n",
    "        output_dir: Directory to save parsed results\n",
    "        language_filter: Process specific language (\"EN\", \"BM\", or \"all\")\n",
    "        max_files: Maximum number of files to process per language\n",
    "        max_pages_per_file: Maximum pages per file (None for all pages)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ›ï¸  MALAYSIAN LEGAL ACTS BATCH PROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Discover files\n",
    "    discovered_files = discover_malaysian_acts(acts_dir)\n",
    "    if not any(discovered_files.values()):\n",
    "        print(\"âŒ No PDF files found. Please check your folder structure.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Setup output directory\n",
    "    parsed_path = setup_parsed_output_directory(output_dir)\n",
    "    \n",
    "    # Step 3: Configure AI models\n",
    "    use_openai, use_gemini = configure_malaysian_legal_analysis()\n",
    "    if use_openai is None:\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Filter files based on language preference\n",
    "    files_to_process = []\n",
    "    \n",
    "    if language_filter == \"all\":\n",
    "        for lang in ['EN', 'BM', 'other']:\n",
    "            lang_files = discovered_files[lang]\n",
    "            if max_files:\n",
    "                lang_files = lang_files[:max_files]\n",
    "            for file_path in lang_files:\n",
    "                files_to_process.append((file_path, lang))\n",
    "    elif language_filter in discovered_files:\n",
    "        lang_files = discovered_files[language_filter]\n",
    "        if max_files:\n",
    "            lang_files = lang_files[:max_files]\n",
    "        for file_path in lang_files:\n",
    "            files_to_process.append((file_path, language_filter))\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(f\"âŒ No files found for language filter: {language_filter}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\\\nðŸŽ¯ PROCESSING PLAN:\")\n",
    "    print(f\"ðŸ“„ Files to process: {len(files_to_process)}\")\n",
    "    print(f\"ðŸŒ Language filter: {language_filter}\")\n",
    "    print(f\"ðŸ“„ Max pages per file: {max_pages_per_file or 'All pages'}\")\n",
    "    \n",
    "    # Step 5: Process files\n",
    "    results = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (file_path, language) in enumerate(files_to_process):\n",
    "        file_name = Path(file_path).name\n",
    "        print(f\"\\\\nðŸ“„ Processing {i+1}/{len(files_to_process)}: {file_name} ({language})\")\n",
    "        \n",
    "        try:\n",
    "            # Determine output file path\n",
    "            output_file = parsed_path / language / f\"{Path(file_path).stem}_analysis.json\"\n",
    "            \n",
    "            # Skip if already processed (basic caching)\n",
    "            if output_file.exists():\n",
    "                print(f\"   â­ï¸  Already processed, loading from cache...\")\n",
    "                with open(output_file, 'r', encoding='utf-8') as f:\n",
    "                    file_result = json.load(f)\n",
    "                results[file_name] = file_result\n",
    "                continue\n",
    "            \n",
    "            # Process with AI labeling\n",
    "            file_result = process_pdf_with_ai_labeling(\n",
    "                pdf_path=file_path,\n",
    "                max_pages=max_pages_per_file,\n",
    "                preferred_model=\"openai\" if use_openai else \"gemini\",\n",
    "                rate_limit_delay=1.0,\n",
    "                batch_size=5\n",
    "            )\n",
    "            \n",
    "            if file_result and 'document_info' in file_result:\n",
    "                # Add language metadata\n",
    "                file_result['document_info']['language'] = language\n",
    "                file_result['document_info']['source_folder'] = str(Path(file_path).parent)\n",
    "                \n",
    "                # Save to parsed folder\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(file_result, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                results[file_name] = file_result\n",
    "                \n",
    "                # Display quick summary\n",
    "                info = file_result['document_info']\n",
    "                summary = file_result['analysis_summary']\n",
    "                print(f\"   âœ… Processed: {info['total_elements']} elements in {info['processing_time_seconds']}s\")\n",
    "                print(f\"   ðŸ“Š High importance: {summary['high_importance_elements']} elements\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"   âŒ Processing failed for {file_name}\")\n",
    "                results[file_name] = {\"error\": \"Processing failed\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error processing {file_name}: {str(e)}\")\n",
    "            results[file_name] = {\"error\": str(e)}\n",
    "    \n",
    "    # Step 6: Generate batch summary\n",
    "    total_time = time.time() - start_time\n",
    "    successful = sum(1 for r in results.values() if \"error\" not in r)\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(f\"\\\\nðŸŽ‰ BATCH PROCESSING COMPLETED!\")\n",
    "    print(f\"âœ… Successful: {successful}/{len(files_to_process)}\")\n",
    "    print(f\"âŒ Failed: {failed}\")\n",
    "    print(f\"â±ï¸  Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"\udcc1 Results saved in: {output_dir}\")\n",
    "    \n",
    "    # Save batch summary\n",
    "    batch_summary = {\n",
    "        'batch_info': {\n",
    "            'total_files': len(files_to_process),\n",
    "            'successful': successful,\n",
    "            'failed': failed,\n",
    "            'total_processing_time': round(total_time, 2),\n",
    "            'language_filter': language_filter,\n",
    "            'processed_at': datetime.now().isoformat()\n",
    "        },\n",
    "        'file_results': results\n",
    "    }\n",
    "    \n",
    "    summary_file = parsed_path / f\"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(batch_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"ðŸ“Š Batch summary saved: {summary_file}\")\n",
    "    \n",
    "    return batch_summary\n",
    "\n",
    "# ðŸ’¡ Quick start examples for Malaysian legal documents\n",
    "def show_malaysian_acts_examples():\n",
    "    \"\"\"Show practical usage examples for Malaysian legal documents\"\"\"\n",
    "    \n",
    "    print(\"ðŸ’¡ MALAYSIAN LEGAL ACTS - USAGE EXAMPLES:\")\n",
    "    print(\"\\\\n1. ðŸ“¤ Process All Documents:\")\n",
    "    print(\"   results = await process_malaysian_acts_folder()\")\n",
    "    \n",
    "    print(\"\\\\n2. \uddecðŸ‡§ English Documents Only:\")\n",
    "    print(\"   results = await process_malaysian_acts_folder(language_filter='EN')\")\n",
    "    \n",
    "    print(\"\\\\n3. ðŸ‡²ðŸ‡¾ Bahasa Malaysia Documents Only:\")\n",
    "    print(\"   results = await process_malaysian_acts_folder(language_filter='BM')\")\n",
    "    \n",
    "    print(\"\\\\n4. ðŸŽ›ï¸  Limited Processing (for testing):\")\n",
    "    print(\"   results = await process_malaysian_acts_folder(\")\n",
    "    print(\"       language_filter='EN',\")\n",
    "    print(\"       max_files=5,\")\n",
    "    print(\"       max_pages_per_file=10\")\n",
    "    print(\"   )\")\n",
    "    \n",
    "    print(\"\\\\n5. \udcc1 Custom Directories:\")\n",
    "    print(\"   results = await process_malaysian_acts_folder(\")\n",
    "    print(\"       acts_dir='/content/my_acts',\")\n",
    "    print(\"       output_dir='/content/my_parsed'\")\n",
    "    print(\"   )\")\n",
    "    \n",
    "    print(\"\\\\n6. ðŸ” Discover Files Only:\")\n",
    "    print(\"   files = discover_malaysian_acts('/content/malaysian_acts')\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "show_malaysian_acts_examples()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"\udfdbï¸  READY TO PROCESS MALAYSIAN LEGAL ACTS!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\\\nðŸ“‚ Expected folder structure:\")\n",
    "print(\"   ðŸ“‚ malaysian_acts/\")\n",
    "print(\"      ðŸ“‚ EN/ (English legal documents)\")\n",
    "print(\"      ðŸ“‚ BM/ (Bahasa Malaysia legal documents)\")\n",
    "print(\"\\\\nðŸ“‚ Output will be saved to:\")\n",
    "print(\"   ðŸ“‚ parsed/\")\n",
    "print(\"      ðŸ“‚ EN/ (English analysis results)\")\n",
    "print(\"      ðŸ“‚ BM/ (Bahasa Malaysia analysis results)\")\n",
    "print(\"\\\\nâš¡ To start processing:\")\n",
    "print(\"   # Process all documents\")\n",
    "print(\"   results = await process_malaysian_acts_folder()\")\n",
    "print(\"\\\\n   # Process English only\")\n",
    "print(\"   results = await process_malaysian_acts_folder(language_filter='EN')\")\n",
    "print(\"\\\\n   # Test with limited files\")\n",
    "print(\"   results = await process_malaysian_acts_folder(max_files=3, max_pages_per_file=5)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
